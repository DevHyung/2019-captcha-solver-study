{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_MNIST.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"7lj_aHT6eJgj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"df4a1b29-5f59-4760-bb9b-ad8767531343","executionInfo":{"status":"ok","timestamp":1554878782781,"user_tz":-540,"elapsed":775,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.utils.data as utildata\n","import torchvision.datasets as ds\n","import torchvision.transforms as transforms\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n","import os"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"Eagel_VQglDZ","colab_type":"code","colab":{}},"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # device 선택\n","# init MNIST dataset\n","trainSet = ds.MNIST(root='./data/',\n","                    train=True,\n","                    transform=transforms.ToTensor(),\n","                    download=True)\n","\n","testSet = ds.MNIST(root='./data/',\n","                   train=False,\n","                   transform=transforms.ToTensor(),\n","                   download=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H9kw5RDYg5b4","colab_type":"code","colab":{}},"cell_type":"code","source":["# hyper param\n","train_epochs = 10 # train 몇번 할거냐\n","learning_rate = 0.001 # 옵티마이저에서 iteration 마다 이동하는 속도?? 뭐라고 해야하지 쨌든 너무 크면 위로 날아가고 너무 작으면 최소 로스 찾기전에 끝남\n","batch_size = 20 # 전체 데이터에서 일정 수의 샘플로 나눠서 weight 갱신 (여기선 20번마다)\n","classes = 10 # 0~9"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cyK0FZGCiL_H","colab_type":"code","colab":{}},"cell_type":"code","source":["# data loader\n","train = utildata.DataLoader(dataset=trainSet, # 불러올 데이타셋\n","                           batch_size=batch_size,\n","                           shuffle=True, # 매 epoch마다 데이터가 재배열 -> 매 epoch마다 dataset이 섞이기 때문에 overfitting을 피할 수 있음\n","                           num_workers=4) # 데이터 불러올때 서브 프로세스 몇개 돌릴거냐? 0은 메인 프로세스에서만\n","test = utildata.DataLoader(dataset=testSet,\n","                           batch_size=batch_size,\n","                           shuffle=False,\n","                           num_workers=4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wcg1kl9jhTlM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Deep NN Midel\n","class myCNN(nn.Module):\n","  def __init__(self, classes = 10):\n","    super(myCNN, self).__init__()\n","    self.layer1 = nn.Sequential(\n","        nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n","        nn.BatchNorm2d(16),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.layer2 = nn.Sequential(\n","        nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","        nn.BatchNorm2d(32),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.fc = nn.Linear(7*7*32, classes)\n","  def forward(self, input):\n","    res = self.layer1(input)\n","    res = self.layer2(res)\n","    res = res.reshape(res.size(0), -1)\n","    res = self.fc(res)\n","    return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0hCndeG-kzSZ","colab_type":"code","outputId":"5907354a-9d27-4f0d-d1b7-ccb98412b14a","executionInfo":{"status":"ok","timestamp":1554878800488,"user_tz":-540,"elapsed":700,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"cell_type":"code","source":["model = myCNN(classes).to(device)\n","print(model)\n","# optimizer (Gradient Descent, Stochastic Gradientt Descent,\n","# Adaptive Gradient, Adaptive Delta, Adaptive Moment Estimation ... )\n","sgd = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","adagrad = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n","adadelta = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n","adam = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["myCNN(\n","  (layer1): Sequential(\n","    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (layer2): Sequential(\n","    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"_qYpywswoEZi","colab_type":"code","colab":{}},"cell_type":"code","source":["# loss function (negative log-likelihood)\n","crossEn = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"96ALArnY8-Oq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":575},"outputId":"c5db2315-072c-492c-a042-55b3f755e7a1","executionInfo":{"status":"ok","timestamp":1554879318261,"user_tz":-540,"elapsed":228716,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["import time\n","\n","st = time.time()\n","for epoch in range(train_epochs):\n","    for i, (images,labels) in enumerate(train):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      predic = model(images)\n","      loss = crossEn(predic,labels)\n","      adam.zero_grad()\n","      loss.backward()\n","      adam.step()\n","\n","      if(i+1)%1000 == 0:\n","        print('Epoch: [{}/{}], Loss: {:.4f}, Step: {}'.format(epoch+1, train_epochs,loss.item(),i+1))\n","print(\"--- %s seconds ---\" %(time.time() - st))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Epoch: [1/10], Loss: 0.0000, Step: 1000\n","Epoch: [1/10], Loss: 0.0104, Step: 2000\n","Epoch: [1/10], Loss: 0.0001, Step: 3000\n","Epoch: [2/10], Loss: 0.0001, Step: 1000\n","Epoch: [2/10], Loss: 0.0000, Step: 2000\n","Epoch: [2/10], Loss: 0.1420, Step: 3000\n","Epoch: [3/10], Loss: 0.0000, Step: 1000\n","Epoch: [3/10], Loss: 0.0000, Step: 2000\n","Epoch: [3/10], Loss: 0.0017, Step: 3000\n","Epoch: [4/10], Loss: 0.0003, Step: 1000\n","Epoch: [4/10], Loss: 0.0000, Step: 2000\n","Epoch: [4/10], Loss: 0.0001, Step: 3000\n","Epoch: [5/10], Loss: 0.0001, Step: 1000\n","Epoch: [5/10], Loss: 0.0078, Step: 2000\n","Epoch: [5/10], Loss: 0.0001, Step: 3000\n","Epoch: [6/10], Loss: 0.0000, Step: 1000\n","Epoch: [6/10], Loss: 0.0085, Step: 2000\n","Epoch: [6/10], Loss: 0.0001, Step: 3000\n","Epoch: [7/10], Loss: 0.0001, Step: 1000\n","Epoch: [7/10], Loss: 0.0000, Step: 2000\n","Epoch: [7/10], Loss: 0.0001, Step: 3000\n","Epoch: [8/10], Loss: 0.0000, Step: 1000\n","Epoch: [8/10], Loss: 0.0000, Step: 2000\n","Epoch: [8/10], Loss: 0.0000, Step: 3000\n","Epoch: [9/10], Loss: 0.0000, Step: 1000\n","Epoch: [9/10], Loss: 0.0001, Step: 2000\n","Epoch: [9/10], Loss: 0.0000, Step: 3000\n","Epoch: [10/10], Loss: 0.0000, Step: 1000\n","Epoch: [10/10], Loss: 0.0000, Step: 2000\n","Epoch: [10/10], Loss: 0.0003, Step: 3000\n","--- 228.01149606704712 seconds ---\n"],"name":"stdout"}]},{"metadata":{"id":"Bc2Vj8rdoHxx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":5417},"outputId":"eb322244-6215-42cc-b595-98c2fcea50ad","executionInfo":{"status":"ok","timestamp":1554873135717,"user_tz":-540,"elapsed":234380,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["if os.path.isfile('./gdrive/My Drive/Colab Notebooks/model/myCNN.pkl'):\n","  # cpu ver\n","  #model.load_state_dict(torch.load('myNN.pkl',map_location='cpu'))\n","  # gpu ver\n","  model.load_state_dict(torch.load('./gdrive/My Drive/Colab Notebooks/model/myCNN.pkl'))\n","else:\n","  for epoch in range(train_epochs):\n","    for i, (images,labels) in enumerate(train):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      predic = model(images)\n","      loss = crossEn(predic,labels)\n","      adam.zero_grad()\n","      loss.backward()\n","      adam.step()\n","\n","      if(i+1)%100 == 0:\n","        print('Epoch: [{}/{}], Loss: {:.4f}, Step: {}'.format(epoch+1, train_epochs,loss.item(),i+1))\n","  torch.save(model.state_dict(), './gdrive/My Drive/Colab Notebooks/model/myCNN.pkl')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch: [1/10], Loss: 0.1200, Step: 100\n","Epoch: [1/10], Loss: 0.2459, Step: 200\n","Epoch: [1/10], Loss: 0.2656, Step: 300\n","Epoch: [1/10], Loss: 0.0315, Step: 400\n","Epoch: [1/10], Loss: 0.0428, Step: 500\n","Epoch: [1/10], Loss: 0.0476, Step: 600\n","Epoch: [1/10], Loss: 0.0822, Step: 700\n","Epoch: [1/10], Loss: 0.0438, Step: 800\n","Epoch: [1/10], Loss: 0.1536, Step: 900\n","Epoch: [1/10], Loss: 0.0363, Step: 1000\n","Epoch: [1/10], Loss: 0.0075, Step: 1100\n","Epoch: [1/10], Loss: 0.1943, Step: 1200\n","Epoch: [1/10], Loss: 0.1242, Step: 1300\n","Epoch: [1/10], Loss: 0.0439, Step: 1400\n","Epoch: [1/10], Loss: 0.0021, Step: 1500\n","Epoch: [1/10], Loss: 0.0085, Step: 1600\n","Epoch: [1/10], Loss: 0.0502, Step: 1700\n","Epoch: [1/10], Loss: 0.0010, Step: 1800\n","Epoch: [1/10], Loss: 0.0103, Step: 1900\n","Epoch: [1/10], Loss: 0.1109, Step: 2000\n","Epoch: [1/10], Loss: 0.0059, Step: 2100\n","Epoch: [1/10], Loss: 0.1442, Step: 2200\n","Epoch: [1/10], Loss: 0.0077, Step: 2300\n","Epoch: [1/10], Loss: 0.0054, Step: 2400\n","Epoch: [1/10], Loss: 0.0009, Step: 2500\n","Epoch: [1/10], Loss: 0.2059, Step: 2600\n","Epoch: [1/10], Loss: 0.3941, Step: 2700\n","Epoch: [1/10], Loss: 0.0525, Step: 2800\n","Epoch: [1/10], Loss: 0.0030, Step: 2900\n","Epoch: [1/10], Loss: 0.0565, Step: 3000\n","Epoch: [2/10], Loss: 0.0214, Step: 100\n","Epoch: [2/10], Loss: 0.2087, Step: 200\n","Epoch: [2/10], Loss: 0.0255, Step: 300\n","Epoch: [2/10], Loss: 0.0919, Step: 400\n","Epoch: [2/10], Loss: 0.3311, Step: 500\n","Epoch: [2/10], Loss: 0.0162, Step: 600\n","Epoch: [2/10], Loss: 0.0196, Step: 700\n","Epoch: [2/10], Loss: 0.4693, Step: 800\n","Epoch: [2/10], Loss: 0.0099, Step: 900\n","Epoch: [2/10], Loss: 0.0395, Step: 1000\n","Epoch: [2/10], Loss: 0.0097, Step: 1100\n","Epoch: [2/10], Loss: 0.0006, Step: 1200\n","Epoch: [2/10], Loss: 0.0242, Step: 1300\n","Epoch: [2/10], Loss: 0.0023, Step: 1400\n","Epoch: [2/10], Loss: 0.0900, Step: 1500\n","Epoch: [2/10], Loss: 0.0135, Step: 1600\n","Epoch: [2/10], Loss: 0.0027, Step: 1700\n","Epoch: [2/10], Loss: 0.0699, Step: 1800\n","Epoch: [2/10], Loss: 0.0123, Step: 1900\n","Epoch: [2/10], Loss: 0.0023, Step: 2000\n","Epoch: [2/10], Loss: 0.0375, Step: 2100\n","Epoch: [2/10], Loss: 0.0029, Step: 2200\n","Epoch: [2/10], Loss: 0.0677, Step: 2300\n","Epoch: [2/10], Loss: 0.0064, Step: 2400\n","Epoch: [2/10], Loss: 0.0055, Step: 2500\n","Epoch: [2/10], Loss: 0.1209, Step: 2600\n","Epoch: [2/10], Loss: 0.1567, Step: 2700\n","Epoch: [2/10], Loss: 0.0050, Step: 2800\n","Epoch: [2/10], Loss: 0.0684, Step: 2900\n","Epoch: [2/10], Loss: 0.0018, Step: 3000\n","Epoch: [3/10], Loss: 0.2703, Step: 100\n","Epoch: [3/10], Loss: 0.0300, Step: 200\n","Epoch: [3/10], Loss: 0.0074, Step: 300\n","Epoch: [3/10], Loss: 0.0065, Step: 400\n","Epoch: [3/10], Loss: 0.0108, Step: 500\n","Epoch: [3/10], Loss: 0.0132, Step: 600\n","Epoch: [3/10], Loss: 0.3334, Step: 700\n","Epoch: [3/10], Loss: 0.0126, Step: 800\n","Epoch: [3/10], Loss: 0.0036, Step: 900\n","Epoch: [3/10], Loss: 0.0146, Step: 1000\n","Epoch: [3/10], Loss: 0.0008, Step: 1100\n","Epoch: [3/10], Loss: 0.0128, Step: 1200\n","Epoch: [3/10], Loss: 0.0019, Step: 1300\n","Epoch: [3/10], Loss: 0.0013, Step: 1400\n","Epoch: [3/10], Loss: 0.0006, Step: 1500\n","Epoch: [3/10], Loss: 0.0122, Step: 1600\n","Epoch: [3/10], Loss: 0.0651, Step: 1700\n","Epoch: [3/10], Loss: 0.0071, Step: 1800\n","Epoch: [3/10], Loss: 0.0009, Step: 1900\n","Epoch: [3/10], Loss: 0.0002, Step: 2000\n","Epoch: [3/10], Loss: 0.0018, Step: 2100\n","Epoch: [3/10], Loss: 0.0048, Step: 2200\n","Epoch: [3/10], Loss: 0.0009, Step: 2300\n","Epoch: [3/10], Loss: 0.0012, Step: 2400\n","Epoch: [3/10], Loss: 0.0036, Step: 2500\n","Epoch: [3/10], Loss: 0.0590, Step: 2600\n","Epoch: [3/10], Loss: 0.0039, Step: 2700\n","Epoch: [3/10], Loss: 0.0018, Step: 2800\n","Epoch: [3/10], Loss: 0.0806, Step: 2900\n","Epoch: [3/10], Loss: 0.0004, Step: 3000\n","Epoch: [4/10], Loss: 0.0031, Step: 100\n","Epoch: [4/10], Loss: 0.0136, Step: 200\n","Epoch: [4/10], Loss: 0.0421, Step: 300\n","Epoch: [4/10], Loss: 0.1955, Step: 400\n","Epoch: [4/10], Loss: 0.0192, Step: 500\n","Epoch: [4/10], Loss: 0.0009, Step: 600\n","Epoch: [4/10], Loss: 0.0715, Step: 700\n","Epoch: [4/10], Loss: 0.1587, Step: 800\n","Epoch: [4/10], Loss: 0.0099, Step: 900\n","Epoch: [4/10], Loss: 0.0024, Step: 1000\n","Epoch: [4/10], Loss: 0.0231, Step: 1100\n","Epoch: [4/10], Loss: 0.0272, Step: 1200\n","Epoch: [4/10], Loss: 0.0005, Step: 1300\n","Epoch: [4/10], Loss: 0.0022, Step: 1400\n","Epoch: [4/10], Loss: 0.0223, Step: 1500\n","Epoch: [4/10], Loss: 0.0011, Step: 1600\n","Epoch: [4/10], Loss: 0.0029, Step: 1700\n","Epoch: [4/10], Loss: 0.0002, Step: 1800\n","Epoch: [4/10], Loss: 0.0353, Step: 1900\n","Epoch: [4/10], Loss: 0.0026, Step: 2000\n","Epoch: [4/10], Loss: 0.0830, Step: 2100\n","Epoch: [4/10], Loss: 0.0035, Step: 2200\n","Epoch: [4/10], Loss: 0.0022, Step: 2300\n","Epoch: [4/10], Loss: 0.0373, Step: 2400\n","Epoch: [4/10], Loss: 0.0020, Step: 2500\n","Epoch: [4/10], Loss: 0.0064, Step: 2600\n","Epoch: [4/10], Loss: 0.0011, Step: 2700\n","Epoch: [4/10], Loss: 0.0058, Step: 2800\n","Epoch: [4/10], Loss: 0.0235, Step: 2900\n","Epoch: [4/10], Loss: 0.0038, Step: 3000\n","Epoch: [5/10], Loss: 0.0006, Step: 100\n","Epoch: [5/10], Loss: 0.0035, Step: 200\n","Epoch: [5/10], Loss: 0.0019, Step: 300\n","Epoch: [5/10], Loss: 0.0006, Step: 400\n","Epoch: [5/10], Loss: 0.0030, Step: 500\n","Epoch: [5/10], Loss: 0.0290, Step: 600\n","Epoch: [5/10], Loss: 0.0035, Step: 700\n","Epoch: [5/10], Loss: 0.0081, Step: 800\n","Epoch: [5/10], Loss: 0.0011, Step: 900\n","Epoch: [5/10], Loss: 0.0010, Step: 1000\n","Epoch: [5/10], Loss: 0.0004, Step: 1100\n","Epoch: [5/10], Loss: 0.0028, Step: 1200\n","Epoch: [5/10], Loss: 0.0758, Step: 1300\n","Epoch: [5/10], Loss: 0.0300, Step: 1400\n","Epoch: [5/10], Loss: 0.0659, Step: 1500\n","Epoch: [5/10], Loss: 0.0203, Step: 1600\n","Epoch: [5/10], Loss: 0.0001, Step: 1700\n","Epoch: [5/10], Loss: 0.0013, Step: 1800\n","Epoch: [5/10], Loss: 0.0113, Step: 1900\n","Epoch: [5/10], Loss: 0.0383, Step: 2000\n","Epoch: [5/10], Loss: 0.0790, Step: 2100\n","Epoch: [5/10], Loss: 0.0115, Step: 2200\n","Epoch: [5/10], Loss: 0.0013, Step: 2300\n","Epoch: [5/10], Loss: 0.0001, Step: 2400\n","Epoch: [5/10], Loss: 0.0004, Step: 2500\n","Epoch: [5/10], Loss: 0.0036, Step: 2600\n","Epoch: [5/10], Loss: 0.0306, Step: 2700\n","Epoch: [5/10], Loss: 0.0001, Step: 2800\n","Epoch: [5/10], Loss: 0.0079, Step: 2900\n","Epoch: [5/10], Loss: 0.1663, Step: 3000\n","Epoch: [6/10], Loss: 0.0052, Step: 100\n","Epoch: [6/10], Loss: 0.0013, Step: 200\n","Epoch: [6/10], Loss: 0.0001, Step: 300\n","Epoch: [6/10], Loss: 0.0006, Step: 400\n","Epoch: [6/10], Loss: 0.1124, Step: 500\n","Epoch: [6/10], Loss: 0.0127, Step: 600\n","Epoch: [6/10], Loss: 0.0428, Step: 700\n","Epoch: [6/10], Loss: 0.0086, Step: 800\n","Epoch: [6/10], Loss: 0.0130, Step: 900\n","Epoch: [6/10], Loss: 0.0112, Step: 1000\n","Epoch: [6/10], Loss: 0.0017, Step: 1100\n","Epoch: [6/10], Loss: 0.0004, Step: 1200\n","Epoch: [6/10], Loss: 0.0155, Step: 1300\n","Epoch: [6/10], Loss: 0.0003, Step: 1400\n","Epoch: [6/10], Loss: 0.0024, Step: 1500\n","Epoch: [6/10], Loss: 0.0243, Step: 1600\n","Epoch: [6/10], Loss: 0.0003, Step: 1700\n","Epoch: [6/10], Loss: 0.0005, Step: 1800\n","Epoch: [6/10], Loss: 0.0071, Step: 1900\n","Epoch: [6/10], Loss: 0.0001, Step: 2000\n","Epoch: [6/10], Loss: 0.0293, Step: 2100\n","Epoch: [6/10], Loss: 0.0001, Step: 2200\n","Epoch: [6/10], Loss: 0.0002, Step: 2300\n","Epoch: [6/10], Loss: 0.0070, Step: 2400\n","Epoch: [6/10], Loss: 0.0014, Step: 2500\n","Epoch: [6/10], Loss: 0.0086, Step: 2600\n","Epoch: [6/10], Loss: 0.0003, Step: 2700\n","Epoch: [6/10], Loss: 0.0003, Step: 2800\n","Epoch: [6/10], Loss: 0.0032, Step: 2900\n","Epoch: [6/10], Loss: 0.0006, Step: 3000\n","Epoch: [7/10], Loss: 0.2220, Step: 100\n","Epoch: [7/10], Loss: 0.0043, Step: 200\n","Epoch: [7/10], Loss: 0.0006, Step: 300\n","Epoch: [7/10], Loss: 0.0007, Step: 400\n","Epoch: [7/10], Loss: 0.0942, Step: 500\n","Epoch: [7/10], Loss: 0.0030, Step: 600\n","Epoch: [7/10], Loss: 0.0028, Step: 700\n","Epoch: [7/10], Loss: 0.0001, Step: 800\n","Epoch: [7/10], Loss: 0.0009, Step: 900\n","Epoch: [7/10], Loss: 0.0028, Step: 1000\n","Epoch: [7/10], Loss: 0.0013, Step: 1100\n","Epoch: [7/10], Loss: 0.0050, Step: 1200\n","Epoch: [7/10], Loss: 0.0007, Step: 1300\n","Epoch: [7/10], Loss: 0.0012, Step: 1400\n","Epoch: [7/10], Loss: 0.0014, Step: 1500\n","Epoch: [7/10], Loss: 0.0001, Step: 1600\n","Epoch: [7/10], Loss: 0.0010, Step: 1700\n","Epoch: [7/10], Loss: 0.0002, Step: 1800\n","Epoch: [7/10], Loss: 0.0006, Step: 1900\n","Epoch: [7/10], Loss: 0.0252, Step: 2000\n","Epoch: [7/10], Loss: 0.0138, Step: 2100\n","Epoch: [7/10], Loss: 0.0006, Step: 2200\n","Epoch: [7/10], Loss: 0.0106, Step: 2300\n","Epoch: [7/10], Loss: 0.0021, Step: 2400\n","Epoch: [7/10], Loss: 0.0102, Step: 2500\n","Epoch: [7/10], Loss: 0.0100, Step: 2600\n","Epoch: [7/10], Loss: 0.0001, Step: 2700\n","Epoch: [7/10], Loss: 0.0046, Step: 2800\n","Epoch: [7/10], Loss: 0.0027, Step: 2900\n","Epoch: [7/10], Loss: 0.0015, Step: 3000\n","Epoch: [8/10], Loss: 0.0013, Step: 100\n","Epoch: [8/10], Loss: 0.0000, Step: 200\n","Epoch: [8/10], Loss: 0.0007, Step: 300\n","Epoch: [8/10], Loss: 0.0077, Step: 400\n","Epoch: [8/10], Loss: 0.0009, Step: 500\n","Epoch: [8/10], Loss: 0.0046, Step: 600\n","Epoch: [8/10], Loss: 0.0001, Step: 700\n","Epoch: [8/10], Loss: 0.0011, Step: 800\n","Epoch: [8/10], Loss: 0.0019, Step: 900\n","Epoch: [8/10], Loss: 0.0001, Step: 1000\n","Epoch: [8/10], Loss: 0.0081, Step: 1100\n","Epoch: [8/10], Loss: 0.0129, Step: 1200\n","Epoch: [8/10], Loss: 0.0962, Step: 1300\n","Epoch: [8/10], Loss: 0.0006, Step: 1400\n","Epoch: [8/10], Loss: 0.0018, Step: 1500\n","Epoch: [8/10], Loss: 0.0005, Step: 1600\n","Epoch: [8/10], Loss: 0.0048, Step: 1700\n","Epoch: [8/10], Loss: 0.0009, Step: 1800\n","Epoch: [8/10], Loss: 0.0200, Step: 1900\n","Epoch: [8/10], Loss: 0.0002, Step: 2000\n","Epoch: [8/10], Loss: 0.0124, Step: 2100\n","Epoch: [8/10], Loss: 0.0008, Step: 2200\n","Epoch: [8/10], Loss: 0.0021, Step: 2300\n","Epoch: [8/10], Loss: 0.0046, Step: 2400\n","Epoch: [8/10], Loss: 0.0316, Step: 2500\n","Epoch: [8/10], Loss: 0.0078, Step: 2600\n","Epoch: [8/10], Loss: 0.0005, Step: 2700\n","Epoch: [8/10], Loss: 0.0006, Step: 2800\n","Epoch: [8/10], Loss: 0.0006, Step: 2900\n","Epoch: [8/10], Loss: 0.0275, Step: 3000\n","Epoch: [9/10], Loss: 0.0048, Step: 100\n","Epoch: [9/10], Loss: 0.0008, Step: 200\n","Epoch: [9/10], Loss: 0.0071, Step: 300\n","Epoch: [9/10], Loss: 0.0000, Step: 400\n","Epoch: [9/10], Loss: 0.0058, Step: 500\n","Epoch: [9/10], Loss: 0.0008, Step: 600\n","Epoch: [9/10], Loss: 0.0011, Step: 700\n","Epoch: [9/10], Loss: 0.0001, Step: 800\n","Epoch: [9/10], Loss: 0.0026, Step: 900\n","Epoch: [9/10], Loss: 0.0207, Step: 1000\n","Epoch: [9/10], Loss: 0.0023, Step: 1100\n","Epoch: [9/10], Loss: 0.0001, Step: 1200\n","Epoch: [9/10], Loss: 0.0000, Step: 1300\n","Epoch: [9/10], Loss: 0.0024, Step: 1400\n","Epoch: [9/10], Loss: 0.0162, Step: 1500\n","Epoch: [9/10], Loss: 0.0304, Step: 1600\n","Epoch: [9/10], Loss: 0.0000, Step: 1700\n","Epoch: [9/10], Loss: 0.0004, Step: 1800\n","Epoch: [9/10], Loss: 0.0003, Step: 1900\n","Epoch: [9/10], Loss: 0.0040, Step: 2000\n","Epoch: [9/10], Loss: 0.0003, Step: 2100\n","Epoch: [9/10], Loss: 0.0008, Step: 2200\n","Epoch: [9/10], Loss: 0.0000, Step: 2300\n","Epoch: [9/10], Loss: 0.0007, Step: 2400\n","Epoch: [9/10], Loss: 0.0206, Step: 2500\n","Epoch: [9/10], Loss: 0.0055, Step: 2600\n","Epoch: [9/10], Loss: 0.0016, Step: 2700\n","Epoch: [9/10], Loss: 0.0000, Step: 2800\n","Epoch: [9/10], Loss: 0.0046, Step: 2900\n","Epoch: [9/10], Loss: 0.0004, Step: 3000\n","Epoch: [10/10], Loss: 0.0365, Step: 100\n","Epoch: [10/10], Loss: 0.0001, Step: 200\n","Epoch: [10/10], Loss: 0.0041, Step: 300\n","Epoch: [10/10], Loss: 0.0001, Step: 400\n","Epoch: [10/10], Loss: 0.0004, Step: 500\n","Epoch: [10/10], Loss: 0.0038, Step: 600\n","Epoch: [10/10], Loss: 0.0000, Step: 700\n","Epoch: [10/10], Loss: 0.0005, Step: 800\n","Epoch: [10/10], Loss: 0.0289, Step: 900\n","Epoch: [10/10], Loss: 0.0099, Step: 1000\n","Epoch: [10/10], Loss: 0.0004, Step: 1100\n","Epoch: [10/10], Loss: 0.0000, Step: 1200\n","Epoch: [10/10], Loss: 0.0001, Step: 1300\n","Epoch: [10/10], Loss: 0.0005, Step: 1400\n","Epoch: [10/10], Loss: 0.0003, Step: 1500\n","Epoch: [10/10], Loss: 0.0001, Step: 1600\n","Epoch: [10/10], Loss: 0.0006, Step: 1700\n","Epoch: [10/10], Loss: 0.0001, Step: 1800\n","Epoch: [10/10], Loss: 0.0000, Step: 1900\n","Epoch: [10/10], Loss: 0.0027, Step: 2000\n","Epoch: [10/10], Loss: 0.0058, Step: 2100\n","Epoch: [10/10], Loss: 0.0069, Step: 2200\n","Epoch: [10/10], Loss: 0.0000, Step: 2300\n","Epoch: [10/10], Loss: 0.0566, Step: 2400\n","Epoch: [10/10], Loss: 0.0417, Step: 2500\n","Epoch: [10/10], Loss: 0.0001, Step: 2600\n","Epoch: [10/10], Loss: 0.0033, Step: 2700\n","Epoch: [10/10], Loss: 0.0000, Step: 2800\n","Epoch: [10/10], Loss: 0.0122, Step: 2900\n","Epoch: [10/10], Loss: 0.0223, Step: 3000\n"],"name":"stdout"}]},{"metadata":{"id":"l2B9oIXosMXy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"54403ed7-6a31-4036-f79a-d864a1cc7567","executionInfo":{"status":"ok","timestamp":1554874854597,"user_tz":-540,"elapsed":3274,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","  correct = 0\n","  total = 0\n","  for images, labels in test:\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  print('Accuracy test on the 10000 images : {}'.format(100 * correct/total))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Accuracy test on the 10000 images : 99.22\n"],"name":"stdout"}]}]}