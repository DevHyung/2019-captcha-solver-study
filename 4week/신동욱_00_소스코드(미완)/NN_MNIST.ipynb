{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN_MNIST.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Zy7s9DeIaK5g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"ef1857c4-5be3-41a1-958a-380d9bcff560","executionInfo":{"status":"ok","timestamp":1554831932010,"user_tz":-540,"elapsed":1000,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.utils.data as utildata\n","import torchvision.datasets as ds\n","import torchvision.transforms as transforms\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"yHwhIKqnalCq","colab_type":"code","outputId":"4c5e6a36-6b3a-4f80-8b8e-53127a74f6cd","executionInfo":{"status":"ok","timestamp":1554831933740,"user_tz":-540,"elapsed":958,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# init MNIST dataset\n","trainSet = ds.MNIST(root='./data/', # \n","                    train=True,\n","                    transform=transforms.ToTensor(),\n","                    download=True)\n","\n","testSet = ds.MNIST(root='./data/',\n","                   train=False,\n","                   transform=transforms.ToTensor(),\n","                   download=True)\n","print(trainSet.train_data.size())\n","# 60000개나 들어있다.\n","print(testSet.train_data.size())\n","# 여긴 10000개 batch는 대충 20? 100? 200?"],"execution_count":13,"outputs":[{"output_type":"stream","text":["torch.Size([60000, 28, 28])\n","torch.Size([10000, 28, 28])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n","  warnings.warn(\"train_data has been renamed data\")\n"],"name":"stderr"}]},{"metadata":{"id":"nzTNcKM-fL9S","colab_type":"code","colab":{}},"cell_type":"code","source":["# hyper param\n","train_epochs = 10 # train 몇번 할거냐\n","learning_rate = 0.001 # 옵티마이저에서 iteration 마다 이동하는 속도?? 뭐라고 해야하지 쨌든 너무 크면 위로 날아가고 너무 작으면 최소 로스 찾기전에 끝남\n","batch_size = 20 # 전체 데이터에서 일정 수의 샘플로 나눠서 weight 갱신 (여기선 20번마다)\n","classes = 10 # 0~9"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZBvKMxC7dsZ6","colab_type":"code","colab":{}},"cell_type":"code","source":["# data loader\n","train = utildata.DataLoader(dataset=trainSet, # 불러올 데이타셋\n","                           batch_size=batch_size,\n","                           shuffle=True, # 매 epoch마다 데이터가 재배열 -> 매 epoch마다 dataset이 섞이기 때문에 overfitting을 피할 수 있음\n","                           num_workers=4) # 데이터 불러올때 서브 프로세스 몇개 돌릴거냐? 0은 메인 프로세스에서만\n","test = utildata.DataLoader(dataset=testSet,\n","                           batch_size=batch_size,\n","                           shuffle=False,\n","                           num_workers=4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H0w3pWCukyZD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Deep NN Midel\n","class myNN(nn.Module):\n","  def __init__(self, classes = 10):\n","    super(myNN,self).__init__()\n","    self.layer1 = nn.Linear(in_features=784, out_features=512, bias=True)\n","    self.layer2 = nn.Linear(in_features=512, out_features=512, bias=True)\n","    self.layer3 = nn.Linear(in_features=512, out_features=512, bias=True)\n","    self.layer4 = nn.Linear(in_features=512, out_features=512, bias=True)\n","    self.layer5 = nn.Linear(in_features=512, out_features=256, bias=True)\n","    self.layer6 = nn.Linear(in_features=256, out_features=128, bias=True)\n","    self.layer7 = nn.Linear(in_features=128, out_features=64, bias=True)\n","    self.layer8 = nn.Linear(in_features=64, out_features=10, bias=True)\n","  def forward(self, input):\n","    res = nn.functional.relu(self.layer1(input))\n","    res = nn.functional.relu(self.layer2(res))\n","    res = nn.functional.relu(self.layer3(res))\n","    res = nn.functional.relu(self.layer4(res))\n","    res = nn.functional.relu(self.layer5(res))\n","    res = nn.functional.relu(self.layer6(res))\n","    res = nn.functional.relu(self.layer7(res))\n","    res = nn.functional.relu(self.layer8(res))\n","    return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NEWLWfion-JC","colab_type":"code","outputId":"2f0c5b6e-ced5-4e30-9b0d-e2d709e91f73","executionInfo":{"status":"ok","timestamp":1554831940215,"user_tz":-540,"elapsed":561,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"cell_type":"code","source":["model = myNN(classes).to(device)\n","print(model)\n","# optimizer (Gradient Descent, Stochastic Gradientt Descent,\n","# Adaptive Gradient, Adaptive Delta, Adaptive Moment Estimation ... )\n","sgd = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","adagrad = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n","adadelta = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n","adam = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["myNN(\n","  (layer1): Linear(in_features=784, out_features=512, bias=True)\n","  (layer2): Linear(in_features=512, out_features=512, bias=True)\n","  (layer3): Linear(in_features=512, out_features=512, bias=True)\n","  (layer4): Linear(in_features=512, out_features=512, bias=True)\n","  (layer5): Linear(in_features=512, out_features=256, bias=True)\n","  (layer6): Linear(in_features=256, out_features=128, bias=True)\n","  (layer7): Linear(in_features=128, out_features=64, bias=True)\n","  (layer8): Linear(in_features=64, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"_gX58OUAqhPq","colab_type":"code","colab":{}},"cell_type":"code","source":["# loss function (negative log-likelihood)\n","crossEn = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lRmcMvrNydt6","colab_type":"code","outputId":"647422a8-8e2c-44d5-95b9-1e5427c62038","executionInfo":{"status":"ok","timestamp":1554825430763,"user_tz":-540,"elapsed":259151,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":5417}},"cell_type":"code","source":["for epoch in range(train_epochs):\n","  for i, (images,labels) in enumerate(train):\n","    images = images.view(-1,784)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    adam.zero_grad()\n","    predic = model(images)\n","    loss = crossEn(predic,labels)\n","    loss.backward()\n","    adam.step()\n","    \n","    if(i+1)%100 == 0:\n","      print('Epoch: [{}/{}], Loss: {:.4f}, Step: {}'.format(epoch+1, train_epochs,loss.item(),i+1))"],"execution_count":68,"outputs":[{"output_type":"stream","text":["Epoch: [1/10], Loss: 0.3454, Step: 100\n","Epoch: [1/10], Loss: 0.2822, Step: 200\n","Epoch: [1/10], Loss: 0.2354, Step: 300\n","Epoch: [1/10], Loss: 0.4625, Step: 400\n","Epoch: [1/10], Loss: 0.1152, Step: 500\n","Epoch: [1/10], Loss: 0.0001, Step: 600\n","Epoch: [1/10], Loss: 0.3458, Step: 700\n","Epoch: [1/10], Loss: 0.3670, Step: 800\n","Epoch: [1/10], Loss: 0.2304, Step: 900\n","Epoch: [1/10], Loss: 0.1241, Step: 1000\n","Epoch: [1/10], Loss: 0.1255, Step: 1100\n","Epoch: [1/10], Loss: 0.3455, Step: 1200\n","Epoch: [1/10], Loss: 0.1084, Step: 1300\n","Epoch: [1/10], Loss: 0.4605, Step: 1400\n","Epoch: [1/10], Loss: 0.2303, Step: 1500\n","Epoch: [1/10], Loss: 0.3454, Step: 1600\n","Epoch: [1/10], Loss: 0.3454, Step: 1700\n","Epoch: [1/10], Loss: 0.1269, Step: 1800\n","Epoch: [1/10], Loss: 0.3570, Step: 1900\n","Epoch: [1/10], Loss: 0.2510, Step: 2000\n","Epoch: [1/10], Loss: 0.1152, Step: 2100\n","Epoch: [1/10], Loss: 0.2303, Step: 2200\n","Epoch: [1/10], Loss: 0.3515, Step: 2300\n","Epoch: [1/10], Loss: 0.2303, Step: 2400\n","Epoch: [1/10], Loss: 0.4676, Step: 2500\n","Epoch: [1/10], Loss: 0.4832, Step: 2600\n","Epoch: [1/10], Loss: 0.4643, Step: 2700\n","Epoch: [1/10], Loss: 0.3460, Step: 2800\n","Epoch: [1/10], Loss: 0.2303, Step: 2900\n","Epoch: [1/10], Loss: 0.4687, Step: 3000\n","Epoch: [2/10], Loss: 0.3454, Step: 100\n","Epoch: [2/10], Loss: 0.4709, Step: 200\n","Epoch: [2/10], Loss: 0.2496, Step: 300\n","Epoch: [2/10], Loss: 0.1274, Step: 400\n","Epoch: [2/10], Loss: 0.3454, Step: 500\n","Epoch: [2/10], Loss: 0.2335, Step: 600\n","Epoch: [2/10], Loss: 0.3535, Step: 700\n","Epoch: [2/10], Loss: 0.2328, Step: 800\n","Epoch: [2/10], Loss: 0.2304, Step: 900\n","Epoch: [2/10], Loss: 0.3936, Step: 1000\n","Epoch: [2/10], Loss: 0.4618, Step: 1100\n","Epoch: [2/10], Loss: 0.6046, Step: 1200\n","Epoch: [2/10], Loss: 0.3455, Step: 1300\n","Epoch: [2/10], Loss: 0.0000, Step: 1400\n","Epoch: [2/10], Loss: 0.1395, Step: 1500\n","Epoch: [2/10], Loss: 0.0001, Step: 1600\n","Epoch: [2/10], Loss: 0.5757, Step: 1700\n","Epoch: [2/10], Loss: 0.5758, Step: 1800\n","Epoch: [2/10], Loss: 0.3454, Step: 1900\n","Epoch: [2/10], Loss: 0.3458, Step: 2000\n","Epoch: [2/10], Loss: 0.3454, Step: 2100\n","Epoch: [2/10], Loss: 0.1152, Step: 2200\n","Epoch: [2/10], Loss: 0.3454, Step: 2300\n","Epoch: [2/10], Loss: 0.1151, Step: 2400\n","Epoch: [2/10], Loss: 0.3454, Step: 2500\n","Epoch: [2/10], Loss: 0.0020, Step: 2600\n","Epoch: [2/10], Loss: 0.4606, Step: 2700\n","Epoch: [2/10], Loss: 0.3482, Step: 2800\n","Epoch: [2/10], Loss: 0.3477, Step: 2900\n","Epoch: [2/10], Loss: 0.1680, Step: 3000\n","Epoch: [3/10], Loss: 0.1424, Step: 100\n","Epoch: [3/10], Loss: 0.2309, Step: 200\n","Epoch: [3/10], Loss: 0.4748, Step: 300\n","Epoch: [3/10], Loss: 0.3454, Step: 400\n","Epoch: [3/10], Loss: 0.3550, Step: 500\n","Epoch: [3/10], Loss: 0.0122, Step: 600\n","Epoch: [3/10], Loss: 0.1158, Step: 700\n","Epoch: [3/10], Loss: 0.2308, Step: 800\n","Epoch: [3/10], Loss: 0.0000, Step: 900\n","Epoch: [3/10], Loss: 0.3459, Step: 1000\n","Epoch: [3/10], Loss: 0.3157, Step: 1100\n","Epoch: [3/10], Loss: 0.2303, Step: 1200\n","Epoch: [3/10], Loss: 0.1246, Step: 1300\n","Epoch: [3/10], Loss: 0.0000, Step: 1400\n","Epoch: [3/10], Loss: 0.0012, Step: 1500\n","Epoch: [3/10], Loss: 0.0097, Step: 1600\n","Epoch: [3/10], Loss: 0.2303, Step: 1700\n","Epoch: [3/10], Loss: 0.1151, Step: 1800\n","Epoch: [3/10], Loss: 0.3521, Step: 1900\n","Epoch: [3/10], Loss: 0.3455, Step: 2000\n","Epoch: [3/10], Loss: 0.7341, Step: 2100\n","Epoch: [3/10], Loss: 0.2303, Step: 2200\n","Epoch: [3/10], Loss: 0.3488, Step: 2300\n","Epoch: [3/10], Loss: 0.2303, Step: 2400\n","Epoch: [3/10], Loss: 0.2090, Step: 2500\n","Epoch: [3/10], Loss: 0.1412, Step: 2600\n","Epoch: [3/10], Loss: 0.4685, Step: 2700\n","Epoch: [3/10], Loss: 0.3209, Step: 2800\n","Epoch: [3/10], Loss: 0.3493, Step: 2900\n","Epoch: [3/10], Loss: 0.1155, Step: 3000\n","Epoch: [4/10], Loss: 0.2307, Step: 100\n","Epoch: [4/10], Loss: 0.0025, Step: 200\n","Epoch: [4/10], Loss: 0.2303, Step: 300\n","Epoch: [4/10], Loss: 0.1151, Step: 400\n","Epoch: [4/10], Loss: 0.1217, Step: 500\n","Epoch: [4/10], Loss: 0.1155, Step: 600\n","Epoch: [4/10], Loss: 0.2337, Step: 700\n","Epoch: [4/10], Loss: 0.2303, Step: 800\n","Epoch: [4/10], Loss: 0.1151, Step: 900\n","Epoch: [4/10], Loss: 0.3454, Step: 1000\n","Epoch: [4/10], Loss: 0.2303, Step: 1100\n","Epoch: [4/10], Loss: 0.2306, Step: 1200\n","Epoch: [4/10], Loss: 0.4605, Step: 1300\n","Epoch: [4/10], Loss: 0.6910, Step: 1400\n","Epoch: [4/10], Loss: 0.2385, Step: 1500\n","Epoch: [4/10], Loss: 0.5757, Step: 1600\n","Epoch: [4/10], Loss: 0.8059, Step: 1700\n","Epoch: [4/10], Loss: 0.6981, Step: 1800\n","Epoch: [4/10], Loss: 0.4606, Step: 1900\n","Epoch: [4/10], Loss: 0.2309, Step: 2000\n","Epoch: [4/10], Loss: 0.9217, Step: 2100\n","Epoch: [4/10], Loss: 0.5135, Step: 2200\n","Epoch: [4/10], Loss: 0.4679, Step: 2300\n","Epoch: [4/10], Loss: 0.5756, Step: 2400\n","Epoch: [4/10], Loss: 0.2312, Step: 2500\n","Epoch: [4/10], Loss: 0.6908, Step: 2600\n","Epoch: [4/10], Loss: 0.0002, Step: 2700\n","Epoch: [4/10], Loss: 0.4606, Step: 2800\n","Epoch: [4/10], Loss: 0.3607, Step: 2900\n","Epoch: [4/10], Loss: 0.5767, Step: 3000\n","Epoch: [5/10], Loss: 0.5757, Step: 100\n","Epoch: [5/10], Loss: 0.3454, Step: 200\n","Epoch: [5/10], Loss: 0.4606, Step: 300\n","Epoch: [5/10], Loss: 0.5757, Step: 400\n","Epoch: [5/10], Loss: 0.6814, Step: 500\n","Epoch: [5/10], Loss: 0.3663, Step: 600\n","Epoch: [5/10], Loss: 0.2598, Step: 700\n","Epoch: [5/10], Loss: 0.5938, Step: 800\n","Epoch: [5/10], Loss: 0.7085, Step: 900\n","Epoch: [5/10], Loss: 0.5758, Step: 1000\n","Epoch: [5/10], Loss: 0.6024, Step: 1100\n","Epoch: [5/10], Loss: 0.3473, Step: 1200\n","Epoch: [5/10], Loss: 0.5870, Step: 1300\n","Epoch: [5/10], Loss: 0.8216, Step: 1400\n","Epoch: [5/10], Loss: 0.9212, Step: 1500\n","Epoch: [5/10], Loss: 0.4607, Step: 1600\n","Epoch: [5/10], Loss: 0.6908, Step: 1700\n","Epoch: [5/10], Loss: 0.6908, Step: 1800\n","Epoch: [5/10], Loss: 0.4614, Step: 1900\n","Epoch: [5/10], Loss: 1.0363, Step: 2000\n","Epoch: [5/10], Loss: 0.5760, Step: 2100\n","Epoch: [5/10], Loss: 0.4630, Step: 2200\n","Epoch: [5/10], Loss: 0.7011, Step: 2300\n","Epoch: [5/10], Loss: 0.7995, Step: 2400\n","Epoch: [5/10], Loss: 0.5398, Step: 2500\n","Epoch: [5/10], Loss: 0.3510, Step: 2600\n","Epoch: [5/10], Loss: 0.3460, Step: 2700\n","Epoch: [5/10], Loss: 0.4606, Step: 2800\n","Epoch: [5/10], Loss: 0.3675, Step: 2900\n","Epoch: [5/10], Loss: 0.1274, Step: 3000\n","Epoch: [6/10], Loss: 0.3472, Step: 100\n","Epoch: [6/10], Loss: 0.2377, Step: 200\n","Epoch: [6/10], Loss: 0.3456, Step: 300\n","Epoch: [6/10], Loss: 0.2303, Step: 400\n","Epoch: [6/10], Loss: 0.1301, Step: 500\n","Epoch: [6/10], Loss: 0.3713, Step: 600\n","Epoch: [6/10], Loss: 0.3456, Step: 700\n","Epoch: [6/10], Loss: 0.4064, Step: 800\n","Epoch: [6/10], Loss: 0.3454, Step: 900\n","Epoch: [6/10], Loss: 0.2476, Step: 1000\n","Epoch: [6/10], Loss: 0.0005, Step: 1100\n","Epoch: [6/10], Loss: 0.1163, Step: 1200\n","Epoch: [6/10], Loss: 0.1154, Step: 1300\n","Epoch: [6/10], Loss: 0.2311, Step: 1400\n","Epoch: [6/10], Loss: 0.2360, Step: 1500\n","Epoch: [6/10], Loss: 0.3733, Step: 1600\n","Epoch: [6/10], Loss: 0.1158, Step: 1700\n","Epoch: [6/10], Loss: 0.2566, Step: 1800\n","Epoch: [6/10], Loss: 0.1666, Step: 1900\n","Epoch: [6/10], Loss: 0.3519, Step: 2000\n","Epoch: [6/10], Loss: 0.5528, Step: 2100\n","Epoch: [6/10], Loss: 0.3455, Step: 2200\n","Epoch: [6/10], Loss: 0.0013, Step: 2300\n","Epoch: [6/10], Loss: 0.1152, Step: 2400\n","Epoch: [6/10], Loss: 0.4631, Step: 2500\n","Epoch: [6/10], Loss: 0.2306, Step: 2600\n","Epoch: [6/10], Loss: 0.2772, Step: 2700\n","Epoch: [6/10], Loss: 0.3454, Step: 2800\n","Epoch: [6/10], Loss: 0.2303, Step: 2900\n","Epoch: [6/10], Loss: 0.2857, Step: 3000\n","Epoch: [7/10], Loss: 0.2312, Step: 100\n","Epoch: [7/10], Loss: 0.3454, Step: 200\n","Epoch: [7/10], Loss: 0.4605, Step: 300\n","Epoch: [7/10], Loss: 0.6908, Step: 400\n","Epoch: [7/10], Loss: 0.3455, Step: 500\n","Epoch: [7/10], Loss: 0.4606, Step: 600\n","Epoch: [7/10], Loss: 0.4626, Step: 700\n","Epoch: [7/10], Loss: 0.3501, Step: 800\n","Epoch: [7/10], Loss: 0.6908, Step: 900\n","Epoch: [7/10], Loss: 0.5757, Step: 1000\n","Epoch: [7/10], Loss: 0.5756, Step: 1100\n","Epoch: [7/10], Loss: 0.6915, Step: 1200\n","Epoch: [7/10], Loss: 0.2303, Step: 1300\n","Epoch: [7/10], Loss: 0.2303, Step: 1400\n","Epoch: [7/10], Loss: 0.3471, Step: 1500\n","Epoch: [7/10], Loss: 0.8071, Step: 1600\n","Epoch: [7/10], Loss: 0.3454, Step: 1700\n","Epoch: [7/10], Loss: 0.1151, Step: 1800\n","Epoch: [7/10], Loss: 0.0000, Step: 1900\n","Epoch: [7/10], Loss: 0.4613, Step: 2000\n","Epoch: [7/10], Loss: 0.2311, Step: 2100\n","Epoch: [7/10], Loss: 0.3939, Step: 2200\n","Epoch: [7/10], Loss: 0.2303, Step: 2300\n","Epoch: [7/10], Loss: 0.3619, Step: 2400\n","Epoch: [7/10], Loss: 0.1152, Step: 2500\n","Epoch: [7/10], Loss: 0.2303, Step: 2600\n","Epoch: [7/10], Loss: 1.1906, Step: 2700\n","Epoch: [7/10], Loss: 0.2366, Step: 2800\n","Epoch: [7/10], Loss: 0.2312, Step: 2900\n","Epoch: [7/10], Loss: 0.1151, Step: 3000\n","Epoch: [8/10], Loss: 0.4606, Step: 100\n","Epoch: [8/10], Loss: 0.5995, Step: 200\n","Epoch: [8/10], Loss: 0.5757, Step: 300\n","Epoch: [8/10], Loss: 0.3465, Step: 400\n","Epoch: [8/10], Loss: 0.4700, Step: 500\n","Epoch: [8/10], Loss: 0.3454, Step: 600\n","Epoch: [8/10], Loss: 0.4605, Step: 700\n","Epoch: [8/10], Loss: 0.3813, Step: 800\n","Epoch: [8/10], Loss: 0.2405, Step: 900\n","Epoch: [8/10], Loss: 0.4606, Step: 1000\n","Epoch: [8/10], Loss: 0.3454, Step: 1100\n","Epoch: [8/10], Loss: 0.6909, Step: 1200\n","Epoch: [8/10], Loss: 0.3896, Step: 1300\n","Epoch: [8/10], Loss: 0.5757, Step: 1400\n","Epoch: [8/10], Loss: 0.9216, Step: 1500\n","Epoch: [8/10], Loss: 0.4906, Step: 1600\n","Epoch: [8/10], Loss: 1.0362, Step: 1700\n","Epoch: [8/10], Loss: 0.8064, Step: 1800\n","Epoch: [8/10], Loss: 0.4605, Step: 1900\n","Epoch: [8/10], Loss: 0.4809, Step: 2000\n","Epoch: [8/10], Loss: 0.8063, Step: 2100\n","Epoch: [8/10], Loss: 0.1334, Step: 2200\n","Epoch: [8/10], Loss: 0.2304, Step: 2300\n","Epoch: [8/10], Loss: 0.5757, Step: 2400\n","Epoch: [8/10], Loss: 0.2353, Step: 2500\n","Epoch: [8/10], Loss: 0.4605, Step: 2600\n","Epoch: [8/10], Loss: 0.3459, Step: 2700\n","Epoch: [8/10], Loss: 0.1275, Step: 2800\n","Epoch: [8/10], Loss: 0.0302, Step: 2900\n","Epoch: [8/10], Loss: 0.2305, Step: 3000\n","Epoch: [9/10], Loss: 0.3454, Step: 100\n","Epoch: [9/10], Loss: 0.1152, Step: 200\n","Epoch: [9/10], Loss: 0.4606, Step: 300\n","Epoch: [9/10], Loss: 0.1151, Step: 400\n","Epoch: [9/10], Loss: 0.1168, Step: 500\n","Epoch: [9/10], Loss: 0.4622, Step: 600\n","Epoch: [9/10], Loss: 0.3454, Step: 700\n","Epoch: [9/10], Loss: 0.2470, Step: 800\n","Epoch: [9/10], Loss: 0.3648, Step: 900\n","Epoch: [9/10], Loss: 0.0125, Step: 1000\n","Epoch: [9/10], Loss: 0.1151, Step: 1100\n","Epoch: [9/10], Loss: 0.3199, Step: 1200\n","Epoch: [9/10], Loss: 0.0192, Step: 1300\n","Epoch: [9/10], Loss: 0.2303, Step: 1400\n","Epoch: [9/10], Loss: 0.1152, Step: 1500\n","Epoch: [9/10], Loss: 0.2352, Step: 1600\n","Epoch: [9/10], Loss: 0.2714, Step: 1700\n","Epoch: [9/10], Loss: 0.1151, Step: 1800\n","Epoch: [9/10], Loss: 0.0000, Step: 1900\n","Epoch: [9/10], Loss: 0.2312, Step: 2000\n","Epoch: [9/10], Loss: 0.2312, Step: 2100\n","Epoch: [9/10], Loss: 0.0016, Step: 2200\n","Epoch: [9/10], Loss: 0.4606, Step: 2300\n","Epoch: [9/10], Loss: 0.1167, Step: 2400\n","Epoch: [9/10], Loss: 0.5774, Step: 2500\n","Epoch: [9/10], Loss: 0.2304, Step: 2600\n","Epoch: [9/10], Loss: 0.5756, Step: 2700\n","Epoch: [9/10], Loss: 0.3457, Step: 2800\n","Epoch: [9/10], Loss: 0.1418, Step: 2900\n","Epoch: [9/10], Loss: 0.2416, Step: 3000\n","Epoch: [10/10], Loss: 0.4605, Step: 100\n","Epoch: [10/10], Loss: 0.3454, Step: 200\n","Epoch: [10/10], Loss: 0.3478, Step: 300\n","Epoch: [10/10], Loss: 0.2303, Step: 400\n","Epoch: [10/10], Loss: 0.2334, Step: 500\n","Epoch: [10/10], Loss: 0.1152, Step: 600\n","Epoch: [10/10], Loss: 0.2303, Step: 700\n","Epoch: [10/10], Loss: 0.3741, Step: 800\n","Epoch: [10/10], Loss: 0.3541, Step: 900\n","Epoch: [10/10], Loss: 0.4714, Step: 1000\n","Epoch: [10/10], Loss: 0.1296, Step: 1100\n","Epoch: [10/10], Loss: 0.1318, Step: 1200\n","Epoch: [10/10], Loss: 0.1156, Step: 1300\n","Epoch: [10/10], Loss: 0.3455, Step: 1400\n","Epoch: [10/10], Loss: 0.5758, Step: 1500\n","Epoch: [10/10], Loss: 0.1155, Step: 1600\n","Epoch: [10/10], Loss: 0.2303, Step: 1700\n","Epoch: [10/10], Loss: 0.2303, Step: 1800\n","Epoch: [10/10], Loss: 0.2304, Step: 1900\n","Epoch: [10/10], Loss: 0.5089, Step: 2000\n","Epoch: [10/10], Loss: 0.3660, Step: 2100\n","Epoch: [10/10], Loss: 0.4712, Step: 2200\n","Epoch: [10/10], Loss: 0.3586, Step: 2300\n","Epoch: [10/10], Loss: 0.2303, Step: 2400\n","Epoch: [10/10], Loss: 0.4606, Step: 2500\n","Epoch: [10/10], Loss: 0.2303, Step: 2600\n","Epoch: [10/10], Loss: 0.2327, Step: 2700\n","Epoch: [10/10], Loss: 0.1151, Step: 2800\n","Epoch: [10/10], Loss: 0.1153, Step: 2900\n","Epoch: [10/10], Loss: 0.2600, Step: 3000\n"],"name":"stdout"}]},{"metadata":{"id":"Xoy-xnDI_7yE","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.save(model.state_dict(), './gdrive/My Drive/myNN.pkl')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SCpovdAEAIVD","colab_type":"code","outputId":"79607560-861b-4e96-f4db-77a658180515","executionInfo":{"status":"ok","timestamp":1554832063462,"user_tz":-540,"elapsed":10325,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":2381}},"cell_type":"code","source":["model.load_state_dict(torch.load('myNN.pkl'))\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","model.eval()\n","with torch.no_grad():\n","  correct = 0\n","  total = 0\n","  for images, labels in test:\n","      pilTrans = transforms.ToPILImage()\n","      pilImg = pilTrans(images[0])\n","      plt.imshow(pilImg)\n","      a = images[0]\n","      images = images.view(-1,784)\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  print('Accuracy test on the 10000 images : {}'.format(100 * correct/total))\n","  print(a)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Accuracy test on the 10000 images : 97.75\n","tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.1255, 0.8627, 0.9961, 0.9412, 0.4510, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.4235,\n","          0.8510, 0.9922, 0.9922, 0.9490, 0.5098, 0.4235, 0.6706, 0.9451,\n","          0.9451, 0.9451, 0.9451, 0.9451, 0.9451, 0.8902, 0.4000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5725, 0.9294, 0.9922,\n","          0.9922, 0.9922, 0.9922, 0.9804, 0.9490, 0.9922, 0.9961, 0.9922,\n","          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9882, 0.6157,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.9686, 0.9922, 0.9922,\n","          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8275, 0.6824, 0.6784,\n","          0.6784, 0.2353, 0.1529, 0.2667, 0.7255, 0.9922, 0.9922, 0.9922,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.9686, 0.9922, 0.9922,\n","          0.9216, 0.7294, 0.3725, 0.2078, 0.2078, 0.0980, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.1608, 0.9922, 0.9922, 0.9922,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4706, 0.7804, 0.3412,\n","          0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0471, 0.6118, 0.9922, 0.9922, 0.8824,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.2118, 0.9922, 0.9922, 0.9843, 0.3961,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.1059,\n","          0.1059, 0.1059, 0.0510, 0.5686, 0.9922, 0.9922, 0.8863, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.3333, 0.7765, 0.9922,\n","          0.9922, 0.9922, 0.7725, 0.9922, 0.9922, 0.9882, 0.3961, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0588, 0.5490, 0.9922, 0.9961, 0.9922,\n","          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8863, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.1255, 0.4745, 0.2235, 0.8078, 0.9961, 0.9961, 0.5255, 0.5255,\n","          0.6980, 0.9961, 0.9961, 0.9961, 1.0000, 0.3686, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.4863, 0.9922, 0.9686, 0.9490, 0.5451, 0.0510, 0.0000, 0.0000,\n","          0.3686, 0.9922, 0.9922, 0.9922, 0.9922, 0.5843, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569,\n","          0.9725, 0.9922, 0.7412, 0.2824, 0.0000, 0.0000, 0.0000, 0.1569,\n","          0.8784, 0.9922, 0.9922, 0.9922, 0.9922, 0.9804, 0.5686, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4824, 0.9686,\n","          0.9922, 0.8157, 0.0510, 0.0000, 0.0000, 0.0000, 0.1686, 0.8627,\n","          0.9922, 0.9922, 0.9922, 0.8275, 0.9922, 0.9922, 0.9569, 0.1490,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373, 0.9725, 0.9922,\n","          0.9216, 0.3020, 0.0000, 0.0314, 0.2667, 0.5412, 0.9020, 0.9843,\n","          0.9922, 0.9922, 0.3686, 0.0980, 0.5647, 0.7569, 0.9922, 0.7451,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5294, 0.9922, 0.9922,\n","          0.6196, 0.0000, 0.1451, 0.5686, 0.9922, 0.9922, 0.9961, 0.9922,\n","          0.9137, 0.3412, 0.0549, 0.0000, 0.0000, 0.0275, 0.2588, 0.1216,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.4118, 0.9765, 0.9922, 0.6902,\n","          0.1922, 0.2706, 0.8941, 0.9922, 0.9922, 0.9922, 1.0000, 0.6196,\n","          0.1961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9922, 0.9922, 0.6863,\n","          0.9176, 0.9922, 0.9922, 0.9922, 0.9922, 0.9373, 0.6431, 0.0196,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9922, 0.9922, 0.9922,\n","          0.9922, 0.9922, 0.9922, 0.9882, 0.9137, 0.1961, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 0.9686, 0.7137, 0.9922,\n","          0.9922, 0.9922, 0.9098, 0.4196, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","          0.0000, 0.0000, 0.0000, 0.0000]]])\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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