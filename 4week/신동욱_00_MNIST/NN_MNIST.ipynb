{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN_MNIST.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Zy7s9DeIaK5g","colab_type":"code","outputId":"3d7ab4cb-5a86-40c7-a85f-90aaf95f6822","executionInfo":{"status":"ok","timestamp":1555050490070,"user_tz":-540,"elapsed":1050,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.utils.data as utildata\n","import torchvision.datasets as ds\n","import torchvision.transforms as transforms\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"zMsHBj84O4GA","colab_type":"text"},"cell_type":"markdown","source":["# 디바이스 선택과 데이터셋 init\n","---\n","torch.device -> 사용할 device 선택하도록\n","\n","ds.MNIST -> MNIST dataset을 Tensor 타입으로 받으며 없으면 다운로드"]},{"metadata":{"id":"yHwhIKqnalCq","colab_type":"code","outputId":"b10e687d-42fd-41e8-93b3-6c53555ec8e0","executionInfo":{"status":"ok","timestamp":1555050514858,"user_tz":-540,"elapsed":972,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# init MNIST dataset\n","trainSet = ds.MNIST(root='./data/', # \n","                    train=True,\n","                    transform=transforms.ToTensor(),\n","                    download=True)\n","\n","testSet = ds.MNIST(root='./data/',\n","                   train=False,\n","                   transform=transforms.ToTensor(),\n","                   download=True)\n","print(trainSet.train_data.size())\n","# 60000개나 들어있다.\n","print(testSet.train_data.size())\n","# 여긴 10000개 batch는 대충 20? 100? 200?"],"execution_count":2,"outputs":[{"output_type":"stream","text":["torch.Size([60000, 28, 28])\n","torch.Size([10000, 28, 28])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n","  warnings.warn(\"train_data has been renamed data\")\n"],"name":"stderr"}]},{"metadata":{"id":"L8jk5Eo4O58V","colab_type":"text"},"cell_type":"markdown","source":["#Hypoer parameter\n","---\n","일종의 튜닝 옵션\n","\n","train_epochs -> 에포크는 학습 반복 횟수\n","\n","learning_rate -> 기울기 찾을때 이동하는 속도\n","\n","batch_size -> 한번에 불러올 데이터(weight 갱신 주기?)\n","\n","classes -> MNIST는 0~9이므로 10개다.\n"]},{"metadata":{"id":"nzTNcKM-fL9S","colab_type":"code","colab":{}},"cell_type":"code","source":["# hyper param\n","train_epochs = 10 # train 몇번 할거냐\n","learning_rate = 0.001 # 옵티마이저에서 iteration 마다 이동하는 속도?? 뭐라고 해야하지 쨌든 너무 크면 위로 날아가고 너무 작으면 최소 로스 찾기전에 끝남\n","batch_size = 20 # 전체 데이터에서 일정 수의 샘플로 나눠서 weight 갱신 (여기선 20번마다)\n","classes = 10 # 0~9"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g6SHo28uO-Kr","colab_type":"text"},"cell_type":"markdown","source":["# 데이터셋 불러오는 부분\n","---\n","dataset -> 불러올 데이터셋\n","\n","batch_size -> 위 설명대로\n","\n","shuffle -> training할때 마다 데이터가 새로 배열된다. 특정 data에만 종속되어 학습되게하는 overfitting을 피할 수 있다고 함\n","\n","num_workers -> 서브 프로세스 얼마나 돌릴건지"]},{"metadata":{"id":"ZBvKMxC7dsZ6","colab_type":"code","colab":{}},"cell_type":"code","source":["# data loader\n","train = utildata.DataLoader(dataset=trainSet, # 불러올 데이타셋\n","                           batch_size=batch_size,\n","                           shuffle=True, # 매 epoch마다 데이터가 재배열 -> 매 epoch마다 dataset이 섞이기 때문에 overfitting을 피할 수 있음\n","                           num_workers=4) # 데이터 불러올때 서브 프로세스 몇개 돌릴거냐? 0은 메인 프로세스에서만\n","test = utildata.DataLoader(dataset=testSet,\n","                           batch_size=batch_size,\n","                           shuffle=False,\n","                           num_workers=4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NBitJU5JPKg2","colab_type":"text"},"cell_type":"markdown","source":["# DNN 모델 생성\n","---\n","기본적으로 torch.nn 내에 있는 모듈을 상속받아 서브클래스를 만들어 사용한다.\n","\n","__init__에서 사용할 멤버변수를 forward에서는 정방향으로 연산한다.\n","\n","Linear는 말 그대로 선형 함수\n","\n","in_feature는 입력값 out_features는 출력값\n","\n","bias는 실제 결과값과 얼마나 차이나는지\n","\n","relu는 activation 함수\n","\n"]},{"metadata":{"id":"H0w3pWCukyZD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Deep NN Midel\n","class myNN(nn.Module):\n","  def __init__(self, classes = 10):\n","    super(myNN,self).__init__()\n","    self.layer1 = nn.Linear(in_features=784, out_features=512, bias=True)\n","    self.layer2 = nn.Linear(in_features=512, out_features=512, bias=True)\n","    self.layer3 = nn.Linear(in_features=512, out_features=512, bias=True)\n","    self.layer4 = nn.Linear(in_features=512, out_features=512, bias=True)\n","    self.layer5 = nn.Linear(in_features=512, out_features=256, bias=True)\n","    self.layer6 = nn.Linear(in_features=256, out_features=128, bias=True)\n","    self.layer7 = nn.Linear(in_features=128, out_features=64, bias=True)\n","    self.layer8 = nn.Linear(in_features=64, out_features=10, bias=True)\n","  def forward(self, input):\n","    res = nn.functional.relu(self.layer1(input))\n","    res = nn.functional.relu(self.layer2(res))\n","    res = nn.functional.relu(self.layer3(res))\n","    res = nn.functional.relu(self.layer4(res))\n","    res = nn.functional.relu(self.layer5(res))\n","    res = nn.functional.relu(self.layer6(res))\n","    res = nn.functional.relu(self.layer7(res))\n","    res = nn.functional.relu(self.layer8(res))\n","    return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RNDETntyQdec","colab_type":"text"},"cell_type":"markdown","source":["# Optimizer\n","---\n","이제 어떻게 최적의 방향으로 갈건지 정하는 방향이다.\n","\n","실제 label과 predict값이 얼마나 차이가 나느냐를 loss라고 하는데 이 loss를 최소화 하기 위해 어떠한 방향으로? 갈건지 정하는 알고리즘?"]},{"metadata":{"id":"NEWLWfion-JC","colab_type":"code","outputId":"2e703f94-e1a3-4d28-d818-fdc015b7706d","executionInfo":{"status":"ok","timestamp":1555050905213,"user_tz":-540,"elapsed":3424,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"cell_type":"code","source":["model = myNN(classes).to(device)\n","print(model)\n","# optimizer (Gradient Descent, Stochastic Gradientt Descent,\n","# Adaptive Gradient, Adaptive Delta, Adaptive Moment Estimation ... )\n","sgd = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","adagrad = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n","adadelta = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n","adam = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["myNN(\n","  (layer1): Linear(in_features=784, out_features=512, bias=True)\n","  (layer2): Linear(in_features=512, out_features=512, bias=True)\n","  (layer3): Linear(in_features=512, out_features=512, bias=True)\n","  (layer4): Linear(in_features=512, out_features=512, bias=True)\n","  (layer5): Linear(in_features=512, out_features=256, bias=True)\n","  (layer6): Linear(in_features=256, out_features=128, bias=True)\n","  (layer7): Linear(in_features=128, out_features=64, bias=True)\n","  (layer8): Linear(in_features=64, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"gzxHHRMvQfBY","colab_type":"text"},"cell_type":"markdown","source":["# loss function\n","---\n","위에서 loss에 대해 간단하게 말했는데, 이 loss를 구하는 식\n","\n","crossEntropy가 어떻게 하는거지 까먹음"]},{"metadata":{"id":"_gX58OUAqhPq","colab_type":"code","colab":{}},"cell_type":"code","source":["# loss function (negative log-likelihood)\n","crossEn = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jN2M6flTQiax","colab_type":"text"},"cell_type":"markdown","source":["# Train\n","---\n","실제로 학습하는 부분이다. 지금까지 위에서 정의한거 다 쓰는곳임\n","(DNN의 경우 258초 경과)"]},{"metadata":{"id":"lRmcMvrNydt6","colab_type":"code","outputId":"0310b2c5-308a-45b2-b9c6-d5c2b8e6a816","executionInfo":{"status":"ok","timestamp":1555051219892,"user_tz":-540,"elapsed":259023,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":5435}},"cell_type":"code","source":["import time\n","\n","st = time.time()\n","\n","for epoch in range(train_epochs):\n","  for i, (images,labels) in enumerate(train):\n","    images = images.view(-1,784)\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    adam.zero_grad()\n","    predic = model(images)\n","    loss = crossEn(predic,labels)\n","    loss.backward()\n","    adam.step()\n","    \n","    if(i+1)%100 == 0:\n","      print('Epoch: [{}/{}], Loss: {:.4f}, Step: {}'.format(epoch+1, train_epochs,loss.item(),i+1))\n","      \n","print(\"--- %s seconds ---\" %(time.time() - st))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: [1/10], Loss: 1.6004, Step: 100\n","Epoch: [1/10], Loss: 1.4333, Step: 200\n","Epoch: [1/10], Loss: 0.9616, Step: 300\n","Epoch: [1/10], Loss: 0.7867, Step: 400\n","Epoch: [1/10], Loss: 0.0606, Step: 500\n","Epoch: [1/10], Loss: 0.4446, Step: 600\n","Epoch: [1/10], Loss: 0.7448, Step: 700\n","Epoch: [1/10], Loss: 0.3014, Step: 800\n","Epoch: [1/10], Loss: 0.2522, Step: 900\n","Epoch: [1/10], Loss: 0.6456, Step: 1000\n","Epoch: [1/10], Loss: 0.5762, Step: 1100\n","Epoch: [1/10], Loss: 0.1469, Step: 1200\n","Epoch: [1/10], Loss: 0.0156, Step: 1300\n","Epoch: [1/10], Loss: 0.1279, Step: 1400\n","Epoch: [1/10], Loss: 0.3504, Step: 1500\n","Epoch: [1/10], Loss: 0.1709, Step: 1600\n","Epoch: [1/10], Loss: 0.1462, Step: 1700\n","Epoch: [1/10], Loss: 0.1004, Step: 1800\n","Epoch: [1/10], Loss: 0.2127, Step: 1900\n","Epoch: [1/10], Loss: 0.4412, Step: 2000\n","Epoch: [1/10], Loss: 0.3583, Step: 2100\n","Epoch: [1/10], Loss: 0.2674, Step: 2200\n","Epoch: [1/10], Loss: 0.1557, Step: 2300\n","Epoch: [1/10], Loss: 0.1973, Step: 2400\n","Epoch: [1/10], Loss: 0.5835, Step: 2500\n","Epoch: [1/10], Loss: 0.2988, Step: 2600\n","Epoch: [1/10], Loss: 0.3659, Step: 2700\n","Epoch: [1/10], Loss: 0.0385, Step: 2800\n","Epoch: [1/10], Loss: 0.1691, Step: 2900\n","Epoch: [1/10], Loss: 0.1730, Step: 3000\n","Epoch: [2/10], Loss: 0.2546, Step: 100\n","Epoch: [2/10], Loss: 0.2931, Step: 200\n","Epoch: [2/10], Loss: 0.0182, Step: 300\n","Epoch: [2/10], Loss: 0.2156, Step: 400\n","Epoch: [2/10], Loss: 0.1735, Step: 500\n","Epoch: [2/10], Loss: 0.3205, Step: 600\n","Epoch: [2/10], Loss: 0.0223, Step: 700\n","Epoch: [2/10], Loss: 0.2425, Step: 800\n","Epoch: [2/10], Loss: 0.1513, Step: 900\n","Epoch: [2/10], Loss: 0.1152, Step: 1000\n","Epoch: [2/10], Loss: 0.0185, Step: 1100\n","Epoch: [2/10], Loss: 0.0062, Step: 1200\n","Epoch: [2/10], Loss: 0.0568, Step: 1300\n","Epoch: [2/10], Loss: 0.1120, Step: 1400\n","Epoch: [2/10], Loss: 0.1593, Step: 1500\n","Epoch: [2/10], Loss: 0.0128, Step: 1600\n","Epoch: [2/10], Loss: 0.1443, Step: 1700\n","Epoch: [2/10], Loss: 0.4592, Step: 1800\n","Epoch: [2/10], Loss: 0.2889, Step: 1900\n","Epoch: [2/10], Loss: 0.0090, Step: 2000\n","Epoch: [2/10], Loss: 0.9504, Step: 2100\n","Epoch: [2/10], Loss: 0.0603, Step: 2200\n","Epoch: [2/10], Loss: 0.2764, Step: 2300\n","Epoch: [2/10], Loss: 0.0595, Step: 2400\n","Epoch: [2/10], Loss: 0.0356, Step: 2500\n","Epoch: [2/10], Loss: 0.0345, Step: 2600\n","Epoch: [2/10], Loss: 0.0058, Step: 2700\n","Epoch: [2/10], Loss: 0.0222, Step: 2800\n","Epoch: [2/10], Loss: 0.8773, Step: 2900\n","Epoch: [2/10], Loss: 0.0788, Step: 3000\n","Epoch: [3/10], Loss: 0.0322, Step: 100\n","Epoch: [3/10], Loss: 0.0746, Step: 200\n","Epoch: [3/10], Loss: 0.0382, Step: 300\n","Epoch: [3/10], Loss: 0.0071, Step: 400\n","Epoch: [3/10], Loss: 0.0142, Step: 500\n","Epoch: [3/10], Loss: 0.2976, Step: 600\n","Epoch: [3/10], Loss: 0.2949, Step: 700\n","Epoch: [3/10], Loss: 0.1500, Step: 800\n","Epoch: [3/10], Loss: 0.1630, Step: 900\n","Epoch: [3/10], Loss: 0.0122, Step: 1000\n","Epoch: [3/10], Loss: 0.0002, Step: 1100\n","Epoch: [3/10], Loss: 0.0068, Step: 1200\n","Epoch: [3/10], Loss: 0.0651, Step: 1300\n","Epoch: [3/10], Loss: 0.0918, Step: 1400\n","Epoch: [3/10], Loss: 0.0030, Step: 1500\n","Epoch: [3/10], Loss: 0.0022, Step: 1600\n","Epoch: [3/10], Loss: 0.0933, Step: 1700\n","Epoch: [3/10], Loss: 0.3780, Step: 1800\n","Epoch: [3/10], Loss: 0.1813, Step: 1900\n","Epoch: [3/10], Loss: 0.0284, Step: 2000\n","Epoch: [3/10], Loss: 0.1116, Step: 2100\n","Epoch: [3/10], Loss: 0.0473, Step: 2200\n","Epoch: [3/10], Loss: 0.2991, Step: 2300\n","Epoch: [3/10], Loss: 0.0448, Step: 2400\n","Epoch: [3/10], Loss: 0.0674, Step: 2500\n","Epoch: [3/10], Loss: 0.0651, Step: 2600\n","Epoch: [3/10], Loss: 0.0102, Step: 2700\n","Epoch: [3/10], Loss: 0.0026, Step: 2800\n","Epoch: [3/10], Loss: 0.0098, Step: 2900\n","Epoch: [3/10], Loss: 0.0563, Step: 3000\n","Epoch: [4/10], Loss: 0.0084, Step: 100\n","Epoch: [4/10], Loss: 0.1758, Step: 200\n","Epoch: [4/10], Loss: 0.0340, Step: 300\n","Epoch: [4/10], Loss: 0.0875, Step: 400\n","Epoch: [4/10], Loss: 0.0312, Step: 500\n","Epoch: [4/10], Loss: 0.0765, Step: 600\n","Epoch: [4/10], Loss: 0.1678, Step: 700\n","Epoch: [4/10], Loss: 0.3871, Step: 800\n","Epoch: [4/10], Loss: 0.2204, Step: 900\n","Epoch: [4/10], Loss: 0.0076, Step: 1000\n","Epoch: [4/10], Loss: 0.3809, Step: 1100\n","Epoch: [4/10], Loss: 0.0782, Step: 1200\n","Epoch: [4/10], Loss: 0.0334, Step: 1300\n","Epoch: [4/10], Loss: 0.0299, Step: 1400\n","Epoch: [4/10], Loss: 0.1818, Step: 1500\n","Epoch: [4/10], Loss: 0.3255, Step: 1600\n","Epoch: [4/10], Loss: 0.0041, Step: 1700\n","Epoch: [4/10], Loss: 0.0040, Step: 1800\n","Epoch: [4/10], Loss: 0.0211, Step: 1900\n","Epoch: [4/10], Loss: 0.0004, Step: 2000\n","Epoch: [4/10], Loss: 0.0075, Step: 2100\n","Epoch: [4/10], Loss: 0.0686, Step: 2200\n","Epoch: [4/10], Loss: 0.3289, Step: 2300\n","Epoch: [4/10], Loss: 0.0057, Step: 2400\n","Epoch: [4/10], Loss: 0.0003, Step: 2500\n","Epoch: [4/10], Loss: 0.0004, Step: 2600\n","Epoch: [4/10], Loss: 0.1808, Step: 2700\n","Epoch: [4/10], Loss: 0.4649, Step: 2800\n","Epoch: [4/10], Loss: 0.3181, Step: 2900\n","Epoch: [4/10], Loss: 0.0022, Step: 3000\n","Epoch: [5/10], Loss: 0.0090, Step: 100\n","Epoch: [5/10], Loss: 0.1618, Step: 200\n","Epoch: [5/10], Loss: 0.0137, Step: 300\n","Epoch: [5/10], Loss: 0.0057, Step: 400\n","Epoch: [5/10], Loss: 0.0440, Step: 500\n","Epoch: [5/10], Loss: 0.0027, Step: 600\n","Epoch: [5/10], Loss: 0.0000, Step: 700\n","Epoch: [5/10], Loss: 0.1015, Step: 800\n","Epoch: [5/10], Loss: 0.0023, Step: 900\n","Epoch: [5/10], Loss: 0.2345, Step: 1000\n","Epoch: [5/10], Loss: 0.0011, Step: 1100\n","Epoch: [5/10], Loss: 0.0557, Step: 1200\n","Epoch: [5/10], Loss: 0.0098, Step: 1300\n","Epoch: [5/10], Loss: 0.0206, Step: 1400\n","Epoch: [5/10], Loss: 0.1859, Step: 1500\n","Epoch: [5/10], Loss: 0.4184, Step: 1600\n","Epoch: [5/10], Loss: 0.1659, Step: 1700\n","Epoch: [5/10], Loss: 0.0027, Step: 1800\n","Epoch: [5/10], Loss: 0.0393, Step: 1900\n","Epoch: [5/10], Loss: 0.1509, Step: 2000\n","Epoch: [5/10], Loss: 0.0080, Step: 2100\n","Epoch: [5/10], Loss: 0.0028, Step: 2200\n","Epoch: [5/10], Loss: 0.0525, Step: 2300\n","Epoch: [5/10], Loss: 0.0896, Step: 2400\n","Epoch: [5/10], Loss: 0.0033, Step: 2500\n","Epoch: [5/10], Loss: 0.0684, Step: 2600\n","Epoch: [5/10], Loss: 0.1411, Step: 2700\n","Epoch: [5/10], Loss: 0.1497, Step: 2800\n","Epoch: [5/10], Loss: 0.0016, Step: 2900\n","Epoch: [5/10], Loss: 0.0021, Step: 3000\n","Epoch: [6/10], Loss: 0.1135, Step: 100\n","Epoch: [6/10], Loss: 0.1786, Step: 200\n","Epoch: [6/10], Loss: 0.0229, Step: 300\n","Epoch: [6/10], Loss: 0.0081, Step: 400\n","Epoch: [6/10], Loss: 0.0018, Step: 500\n","Epoch: [6/10], Loss: 0.0021, Step: 600\n","Epoch: [6/10], Loss: 0.0057, Step: 700\n","Epoch: [6/10], Loss: 0.2422, Step: 800\n","Epoch: [6/10], Loss: 0.0157, Step: 900\n","Epoch: [6/10], Loss: 0.1287, Step: 1000\n","Epoch: [6/10], Loss: 0.0203, Step: 1100\n","Epoch: [6/10], Loss: 0.1506, Step: 1200\n","Epoch: [6/10], Loss: 0.3682, Step: 1300\n","Epoch: [6/10], Loss: 0.0052, Step: 1400\n","Epoch: [6/10], Loss: 0.0010, Step: 1500\n","Epoch: [6/10], Loss: 0.0033, Step: 1600\n","Epoch: [6/10], Loss: 0.0692, Step: 1700\n","Epoch: [6/10], Loss: 0.0636, Step: 1800\n","Epoch: [6/10], Loss: 0.0595, Step: 1900\n","Epoch: [6/10], Loss: 0.0382, Step: 2000\n","Epoch: [6/10], Loss: 0.0056, Step: 2100\n","Epoch: [6/10], Loss: 0.0176, Step: 2200\n","Epoch: [6/10], Loss: 0.0015, Step: 2300\n","Epoch: [6/10], Loss: 0.0042, Step: 2400\n","Epoch: [6/10], Loss: 0.0065, Step: 2500\n","Epoch: [6/10], Loss: 0.0027, Step: 2600\n","Epoch: [6/10], Loss: 0.0014, Step: 2700\n","Epoch: [6/10], Loss: 0.0229, Step: 2800\n","Epoch: [6/10], Loss: 0.0095, Step: 2900\n","Epoch: [6/10], Loss: 0.2903, Step: 3000\n","Epoch: [7/10], Loss: 0.0458, Step: 100\n","Epoch: [7/10], Loss: 0.1332, Step: 200\n","Epoch: [7/10], Loss: 0.0141, Step: 300\n","Epoch: [7/10], Loss: 0.0489, Step: 400\n","Epoch: [7/10], Loss: 0.0313, Step: 500\n","Epoch: [7/10], Loss: 0.2190, Step: 600\n","Epoch: [7/10], Loss: 0.0001, Step: 700\n","Epoch: [7/10], Loss: 0.0902, Step: 800\n","Epoch: [7/10], Loss: 0.0011, Step: 900\n","Epoch: [7/10], Loss: 0.1806, Step: 1000\n","Epoch: [7/10], Loss: 0.0003, Step: 1100\n","Epoch: [7/10], Loss: 0.0637, Step: 1200\n","Epoch: [7/10], Loss: 0.0513, Step: 1300\n","Epoch: [7/10], Loss: 0.1573, Step: 1400\n","Epoch: [7/10], Loss: 0.0048, Step: 1500\n","Epoch: [7/10], Loss: 0.0008, Step: 1600\n","Epoch: [7/10], Loss: 0.0582, Step: 1700\n","Epoch: [7/10], Loss: 0.0039, Step: 1800\n","Epoch: [7/10], Loss: 0.0382, Step: 1900\n","Epoch: [7/10], Loss: 0.0099, Step: 2000\n","Epoch: [7/10], Loss: 0.0009, Step: 2100\n","Epoch: [7/10], Loss: 0.0680, Step: 2200\n","Epoch: [7/10], Loss: 0.0565, Step: 2300\n","Epoch: [7/10], Loss: 0.0692, Step: 2400\n","Epoch: [7/10], Loss: 0.0087, Step: 2500\n","Epoch: [7/10], Loss: 0.0001, Step: 2600\n","Epoch: [7/10], Loss: 0.0009, Step: 2700\n","Epoch: [7/10], Loss: 0.0945, Step: 2800\n","Epoch: [7/10], Loss: 0.0012, Step: 2900\n","Epoch: [7/10], Loss: 0.0187, Step: 3000\n","Epoch: [8/10], Loss: 0.0017, Step: 100\n","Epoch: [8/10], Loss: 0.0008, Step: 200\n","Epoch: [8/10], Loss: 0.0004, Step: 300\n","Epoch: [8/10], Loss: 0.0006, Step: 400\n","Epoch: [8/10], Loss: 0.0009, Step: 500\n","Epoch: [8/10], Loss: 0.0165, Step: 600\n","Epoch: [8/10], Loss: 0.0115, Step: 700\n","Epoch: [8/10], Loss: 0.0060, Step: 800\n","Epoch: [8/10], Loss: 0.2211, Step: 900\n","Epoch: [8/10], Loss: 0.0003, Step: 1000\n","Epoch: [8/10], Loss: 0.0116, Step: 1100\n","Epoch: [8/10], Loss: 0.0347, Step: 1200\n","Epoch: [8/10], Loss: 0.0080, Step: 1300\n","Epoch: [8/10], Loss: 0.0913, Step: 1400\n","Epoch: [8/10], Loss: 0.0579, Step: 1500\n","Epoch: [8/10], Loss: 0.0023, Step: 1600\n","Epoch: [8/10], Loss: 0.0034, Step: 1700\n","Epoch: [8/10], Loss: 0.1081, Step: 1800\n","Epoch: [8/10], Loss: 0.0004, Step: 1900\n","Epoch: [8/10], Loss: 0.0370, Step: 2000\n","Epoch: [8/10], Loss: 0.0738, Step: 2100\n","Epoch: [8/10], Loss: 0.0002, Step: 2200\n","Epoch: [8/10], Loss: 0.0291, Step: 2300\n","Epoch: [8/10], Loss: 0.3759, Step: 2400\n","Epoch: [8/10], Loss: 0.0004, Step: 2500\n","Epoch: [8/10], Loss: 0.0790, Step: 2600\n","Epoch: [8/10], Loss: 0.0010, Step: 2700\n","Epoch: [8/10], Loss: 0.0204, Step: 2800\n","Epoch: [8/10], Loss: 0.0098, Step: 2900\n","Epoch: [8/10], Loss: 0.0000, Step: 3000\n","Epoch: [9/10], Loss: 0.0026, Step: 100\n","Epoch: [9/10], Loss: 0.0059, Step: 200\n","Epoch: [9/10], Loss: 0.0257, Step: 300\n","Epoch: [9/10], Loss: 0.2411, Step: 400\n","Epoch: [9/10], Loss: 0.0004, Step: 500\n","Epoch: [9/10], Loss: 0.1280, Step: 600\n","Epoch: [9/10], Loss: 0.0191, Step: 700\n","Epoch: [9/10], Loss: 0.0109, Step: 800\n","Epoch: [9/10], Loss: 0.0045, Step: 900\n","Epoch: [9/10], Loss: 0.0256, Step: 1000\n","Epoch: [9/10], Loss: 0.2746, Step: 1100\n","Epoch: [9/10], Loss: 0.0005, Step: 1200\n","Epoch: [9/10], Loss: 0.5905, Step: 1300\n","Epoch: [9/10], Loss: 0.0002, Step: 1400\n","Epoch: [9/10], Loss: 0.0131, Step: 1500\n","Epoch: [9/10], Loss: 0.0146, Step: 1600\n","Epoch: [9/10], Loss: 0.0012, Step: 1700\n","Epoch: [9/10], Loss: 0.7743, Step: 1800\n","Epoch: [9/10], Loss: 0.0015, Step: 1900\n","Epoch: [9/10], Loss: 0.0102, Step: 2000\n","Epoch: [9/10], Loss: 0.0001, Step: 2100\n","Epoch: [9/10], Loss: 0.0005, Step: 2200\n","Epoch: [9/10], Loss: 0.0005, Step: 2300\n","Epoch: [9/10], Loss: 0.5580, Step: 2400\n","Epoch: [9/10], Loss: 0.0270, Step: 2500\n","Epoch: [9/10], Loss: 0.0026, Step: 2600\n","Epoch: [9/10], Loss: 0.0003, Step: 2700\n","Epoch: [9/10], Loss: 0.0010, Step: 2800\n","Epoch: [9/10], Loss: 0.1648, Step: 2900\n","Epoch: [9/10], Loss: 0.1922, Step: 3000\n","Epoch: [10/10], Loss: 0.0005, Step: 100\n","Epoch: [10/10], Loss: 0.0052, Step: 200\n","Epoch: [10/10], Loss: 0.0001, Step: 300\n","Epoch: [10/10], Loss: 0.1141, Step: 400\n","Epoch: [10/10], Loss: 0.0001, Step: 500\n","Epoch: [10/10], Loss: 0.0127, Step: 600\n","Epoch: [10/10], Loss: 0.0001, Step: 700\n","Epoch: [10/10], Loss: 0.0929, Step: 800\n","Epoch: [10/10], Loss: 0.0045, Step: 900\n","Epoch: [10/10], Loss: 0.1968, Step: 1000\n","Epoch: [10/10], Loss: 0.0162, Step: 1100\n","Epoch: [10/10], Loss: 0.3611, Step: 1200\n","Epoch: [10/10], Loss: 0.0003, Step: 1300\n","Epoch: [10/10], Loss: 0.0053, Step: 1400\n","Epoch: [10/10], Loss: 0.0011, Step: 1500\n","Epoch: [10/10], Loss: 0.0001, Step: 1600\n","Epoch: [10/10], Loss: 0.0390, Step: 1700\n","Epoch: [10/10], Loss: 0.0000, Step: 1800\n","Epoch: [10/10], Loss: 0.0028, Step: 1900\n","Epoch: [10/10], Loss: 0.0000, Step: 2000\n","Epoch: [10/10], Loss: 0.0029, Step: 2100\n","Epoch: [10/10], Loss: 0.0006, Step: 2200\n","Epoch: [10/10], Loss: 0.0000, Step: 2300\n","Epoch: [10/10], Loss: 0.0223, Step: 2400\n","Epoch: [10/10], Loss: 0.0200, Step: 2500\n","Epoch: [10/10], Loss: 0.0365, Step: 2600\n","Epoch: [10/10], Loss: 0.1371, Step: 2700\n","Epoch: [10/10], Loss: 0.0043, Step: 2800\n","Epoch: [10/10], Loss: 0.0406, Step: 2900\n","Epoch: [10/10], Loss: 0.0757, Step: 3000\n","--- 258.01980471611023 seconds ---\n"],"name":"stdout"}]},{"metadata":{"id":"Xoy-xnDI_7yE","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.save(model.state_dict(), './gdrive/My Drive/myNN.pkl')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qFjXLk58SAAD","colab_type":"text"},"cell_type":"markdown","source":["# Test\n","---\n","이제 얼마나 잘 학습했는지 test를 하면 된다. (Accr : 97.49)"]},{"metadata":{"id":"SCpovdAEAIVD","colab_type":"code","outputId":"d9ce6312-a7d6-4688-e756-a8c2578f20bf","executionInfo":{"status":"ok","timestamp":1555051308852,"user_tz":-540,"elapsed":3444,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#model.load_state_dict(torch.load('myNN.pkl'))\n","\n","model.eval()\n","with torch.no_grad():\n","  correct = 0\n","  total = 0\n","  for images, labels in test:\n","      images = images.view(-1,784)\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  print('Accuracy test on the 10000 images : {}'.format(100 * correct/total))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Accuracy test on the 10000 images : 97.49\n"],"name":"stdout"}]}]}