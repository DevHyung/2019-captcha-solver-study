{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_MNIST.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"7lj_aHT6eJgj","colab_type":"code","outputId":"6598c1b6-ce71-4ff1-bab1-e1db7d45f65b","executionInfo":{"status":"ok","timestamp":1555048441428,"user_tz":-540,"elapsed":23022,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.utils.data as utildata\n","import torchvision.datasets as ds\n","import torchvision.transforms as transforms\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n","import os"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"UjmD39U4Ev6i","colab_type":"text"},"cell_type":"markdown","source":["# 디바이스 선택과 데이터셋 init\n","---\n","torch.device -> 사용할 device 선택하도록\n","\n","ds.MNIST -> MNIST dataset을 Tensor 타입으로 받으며 없으면 다운로드"]},{"metadata":{"id":"Eagel_VQglDZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"a09c0aa7-a470-46eb-e43c-79467a882094","executionInfo":{"status":"ok","timestamp":1555048535211,"user_tz":-540,"elapsed":985,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # device 선택 (현재 CUDA 사용 가능하면 쓰고, 아니면 cpu로)\n","# init MNIST dataset\n","trainSet = ds.MNIST(root='./data/',\n","                    train=True,\n","                    transform=transforms.ToTensor(),\n","                    download=True)\n","\n","testSet = ds.MNIST(root='./data/',\n","                   train=False,\n","                   transform=transforms.ToTensor(),\n","                   download=True)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Dataset MNIST\n","    Number of datapoints: 60000\n","    Split: train\n","    Root Location: ./data/\n","    Transforms (if any): ToTensor()\n","    Target Transforms (if any): None\n"],"name":"stdout"}]},{"metadata":{"id":"i_4xdvDNFnBg","colab_type":"text"},"cell_type":"markdown","source":["#Hypoer parameter\n","---\n","일종의 튜닝 옵션\n","\n","train_epochs -> 에포크는 학습 반복 횟수\n","\n","learning_rate -> 기울기 찾을때 이동하는 속도\n","\n","batch_size -> 한번에 불러올 데이터(weight 갱신 주기?)\n","\n","classes -> MNIST는 0~9이므로 10개다.\n"]},{"metadata":{"id":"H9kw5RDYg5b4","colab_type":"code","colab":{}},"cell_type":"code","source":["# hyper param\n","train_epochs = 10 # train 몇번 할거냐\n","learning_rate = 0.001 # 옵티마이저에서 iteration 마다 이동하는 속도?? 뭐라고 해야하지 쨌든 너무 크면 위로 날아가고 너무 작으면 최소 로스 찾기전에 끝남\n","batch_size = 20 # 전체 데이터에서 일정 수의 샘플로 나눠서 weight 갱신 (여기선 20번마다)\n","classes = 10 # 0~9"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SPtB5_VLHE05","colab_type":"text"},"cell_type":"markdown","source":["# 데이터셋 불러오는 부분\n","---\n","dataset -> 불러올 데이터셋\n","\n","batch_size -> 위 설명대로\n","\n","shuffle -> training할때 마다 데이터가 새로 배열된다. 특정 data에만 종속되어 학습되게하는 overfitting을 피할 수 있다고 함\n","\n","num_workers -> 서브 프로세스 얼마나 돌릴건지"]},{"metadata":{"id":"cyK0FZGCiL_H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"9a744403-e036-40cc-b013-f81b7df8ebac","executionInfo":{"status":"ok","timestamp":1555048542460,"user_tz":-540,"elapsed":885,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}}},"cell_type":"code","source":["# data loader\n","train = utildata.DataLoader(dataset=trainSet, # 불러올 데이타셋\n","                           batch_size=batch_size,\n","                           shuffle=True, # 매 epoch마다 데이터가 재배열 -> 매 epoch마다 dataset이 섞이기 때문에 overfitting을 피할 수 있음\n","                           num_workers=4) # 데이터 불러올때 서브 프로세스 몇개 돌릴거냐? 0은 메인 프로세스에서만\n","test = utildata.DataLoader(dataset=testSet,\n","                           batch_size=batch_size,\n","                           shuffle=False,\n","                           num_workers=4)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["<torch.utils.data.dataloader.DataLoader object at 0x7fe6bd255cf8>\n"],"name":"stdout"}]},{"metadata":{"id":"Pc3DugiHH3aT","colab_type":"text"},"cell_type":"markdown","source":["# CNN 모델 생성\n","---\n","기본적으로 torch.nn 내에 있는 모듈을 상속받아 서브클래스를 만들어 사용한다.\n","\n","__init__에서 사용할 멤버변수를 forward에서는 정방향으로 연산한다.\n","\n","Sequential 컨테이너는 들어갈 모듈의 순서를 나타낸다.\n","\n","CNN은 기본적으로 (입력 채널 수, 출력 채널 수, 커널 사이즈(5x5), stride(커널 몇칸씩 이동할 건지), padding(좌우상하 2칸씩)) 을 받는다.\n","\n","BatchNorm2d는 activation function에 입력값을 넣기전에 평균이 0으로 이쁜 모양으로 만들어 주기 위한 기법 (기울기 실종 등 잡아줌)\n","\n","ReLU는 Activation function\n","\n","MaxPool2d는 이제 Convolution layer가 끝난 뒤 풀링하는 단계다 maxpooling이면 가장 큰 값 풀링\n","\n","fc는 fully connected layer로 cnn을 통해 feature값을 생성하면 합쳐서 classification"]},{"metadata":{"id":"wcg1kl9jhTlM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Convolutional NN Midel\n","class myCNN(nn.Module):\n","  def __init__(self, classes = 10):\n","    super(myCNN, self).__init__()\n","    self.layer1 = nn.Sequential(\n","        nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n","        nn.BatchNorm2d(16),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.layer2 = nn.Sequential(\n","        nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","        nn.BatchNorm2d(32),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.fc = nn.Linear(7*7*32, classes)\n","  def forward(self, input):\n","    res = self.layer1(input)\n","    res = self.layer2(res)\n","    res = res.reshape(res.size(0), -1)\n","    res = self.fc(res)\n","    return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BAfJaV9fK0-X","colab_type":"text"},"cell_type":"markdown","source":["# Optimizer\n","---\n","이제 어떻게 최적의 방향으로 갈건지 정하는 방향이다.\n","\n","실제 label과 predict값이 얼마나 차이가 나느냐를 loss라고 하는데 이 loss를 최소화 하기 위해 어떠한 방향으로? 갈건지 정하는 알고리즘?"]},{"metadata":{"id":"0hCndeG-kzSZ","colab_type":"code","outputId":"27feff5e-a608-411c-96b0-c264e7353e61","executionInfo":{"status":"ok","timestamp":1555049505500,"user_tz":-540,"elapsed":6341,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"cell_type":"code","source":["model = myCNN(classes).to(device)\n","print(model)\n","# optimizer (Gradient Descent, Stochastic Gradientt Descent,\n","# Adaptive Gradient, Adaptive Delta, Adaptive Moment Estimation ... )\n","sgd = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","adagrad = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n","adadelta = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n","adam = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["myCNN(\n","  (layer1): Sequential(\n","    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (layer2): Sequential(\n","    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"OnTlxnTOLQPE","colab_type":"text"},"cell_type":"markdown","source":["# loss function\n","---\n","위에서 loss에 대해 간단하게 말했는데, 이 loss를 구하는 식\n","\n","crossEntropy가 어떻게 하는거지 까먹음"]},{"metadata":{"id":"_qYpywswoEZi","colab_type":"code","colab":{}},"cell_type":"code","source":["# loss function (negative log-likelihood)\n","crossEn = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P-zSf9esLqpy","colab_type":"text"},"cell_type":"markdown","source":["# Train\n","---\n","실제로 학습하는 부분이다. 지금까지 위에서 정의한거 다 쓰는곳임\n","(CNN의 경우 228초 경과)"]},{"metadata":{"id":"96ALArnY8-Oq","colab_type":"code","outputId":"c5db2315-072c-492c-a042-55b3f755e7a1","executionInfo":{"status":"ok","timestamp":1554879318261,"user_tz":-540,"elapsed":228716,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":575}},"cell_type":"code","source":["import time\n","\n","st = time.time()\n","for epoch in range(train_epochs):\n","    for i, (images,labels) in enumerate(train):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      predic = model(images)\n","      loss = crossEn(predic,labels)\n","      adam.zero_grad()\n","      loss.backward()\n","      adam.step()\n","\n","      if(i+1)%1000 == 0:\n","        print('Epoch: [{}/{}], Loss: {:.4f}, Step: {}'.format(epoch+1, train_epochs,loss.item(),i+1))\n","print(\"--- %s seconds ---\" %(time.time() - st))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: [1/10], Loss: 0.0000, Step: 1000\n","Epoch: [1/10], Loss: 0.0104, Step: 2000\n","Epoch: [1/10], Loss: 0.0001, Step: 3000\n","Epoch: [2/10], Loss: 0.0001, Step: 1000\n","Epoch: [2/10], Loss: 0.0000, Step: 2000\n","Epoch: [2/10], Loss: 0.1420, Step: 3000\n","Epoch: [3/10], Loss: 0.0000, Step: 1000\n","Epoch: [3/10], Loss: 0.0000, Step: 2000\n","Epoch: [3/10], Loss: 0.0017, Step: 3000\n","Epoch: [4/10], Loss: 0.0003, Step: 1000\n","Epoch: [4/10], Loss: 0.0000, Step: 2000\n","Epoch: [4/10], Loss: 0.0001, Step: 3000\n","Epoch: [5/10], Loss: 0.0001, Step: 1000\n","Epoch: [5/10], Loss: 0.0078, Step: 2000\n","Epoch: [5/10], Loss: 0.0001, Step: 3000\n","Epoch: [6/10], Loss: 0.0000, Step: 1000\n","Epoch: [6/10], Loss: 0.0085, Step: 2000\n","Epoch: [6/10], Loss: 0.0001, Step: 3000\n","Epoch: [7/10], Loss: 0.0001, Step: 1000\n","Epoch: [7/10], Loss: 0.0000, Step: 2000\n","Epoch: [7/10], Loss: 0.0001, Step: 3000\n","Epoch: [8/10], Loss: 0.0000, Step: 1000\n","Epoch: [8/10], Loss: 0.0000, Step: 2000\n","Epoch: [8/10], Loss: 0.0000, Step: 3000\n","Epoch: [9/10], Loss: 0.0000, Step: 1000\n","Epoch: [9/10], Loss: 0.0001, Step: 2000\n","Epoch: [9/10], Loss: 0.0000, Step: 3000\n","Epoch: [10/10], Loss: 0.0000, Step: 1000\n","Epoch: [10/10], Loss: 0.0000, Step: 2000\n","Epoch: [10/10], Loss: 0.0003, Step: 3000\n","--- 228.01149606704712 seconds ---\n"],"name":"stdout"}]},{"metadata":{"id":"5VNdFfAYL2g6","colab_type":"text"},"cell_type":"markdown","source":["# Train(2)\n","---\n","이건 Colab용으로.."]},{"metadata":{"id":"Bc2Vj8rdoHxx","colab_type":"code","outputId":"eb322244-6215-42cc-b595-98c2fcea50ad","executionInfo":{"status":"ok","timestamp":1554873135717,"user_tz":-540,"elapsed":234380,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":5417}},"cell_type":"code","source":["if os.path.isfile('./gdrive/My Drive/Colab Notebooks/model/myCNN.pkl'):\n","  # cpu ver\n","  #model.load_state_dict(torch.load('myNN.pkl',map_location='cpu'))\n","  # gpu ver\n","  model.load_state_dict(torch.load('./gdrive/My Drive/Colab Notebooks/model/myCNN.pkl'))\n","else:\n","  for epoch in range(train_epochs):\n","    for i, (images,labels) in enumerate(train):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      predic = model(images)\n","      loss = crossEn(predic,labels)\n","      adam.zero_grad()\n","      loss.backward()\n","      adam.step()\n","\n","      if(i+1)%100 == 0:\n","        print('Epoch: [{}/{}], Loss: {:.4f}, Step: {}'.format(epoch+1, train_epochs,loss.item(),i+1))\n","  torch.save(model.state_dict(), './gdrive/My Drive/Colab Notebooks/model/myCNN.pkl')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: [1/10], Loss: 0.1200, Step: 100\n","Epoch: [1/10], Loss: 0.2459, Step: 200\n","Epoch: [1/10], Loss: 0.2656, Step: 300\n","Epoch: [1/10], Loss: 0.0315, Step: 400\n","Epoch: [1/10], Loss: 0.0428, Step: 500\n","Epoch: [1/10], Loss: 0.0476, Step: 600\n","Epoch: [1/10], Loss: 0.0822, Step: 700\n","Epoch: [1/10], Loss: 0.0438, Step: 800\n","Epoch: [1/10], Loss: 0.1536, Step: 900\n","Epoch: [1/10], Loss: 0.0363, Step: 1000\n","Epoch: [1/10], Loss: 0.0075, Step: 1100\n","Epoch: [1/10], Loss: 0.1943, Step: 1200\n","Epoch: [1/10], Loss: 0.1242, Step: 1300\n","Epoch: [1/10], Loss: 0.0439, Step: 1400\n","Epoch: [1/10], Loss: 0.0021, Step: 1500\n","Epoch: [1/10], Loss: 0.0085, Step: 1600\n","Epoch: [1/10], Loss: 0.0502, Step: 1700\n","Epoch: [1/10], Loss: 0.0010, Step: 1800\n","Epoch: [1/10], Loss: 0.0103, Step: 1900\n","Epoch: [1/10], Loss: 0.1109, Step: 2000\n","Epoch: [1/10], Loss: 0.0059, Step: 2100\n","Epoch: [1/10], Loss: 0.1442, Step: 2200\n","Epoch: [1/10], Loss: 0.0077, Step: 2300\n","Epoch: [1/10], Loss: 0.0054, Step: 2400\n","Epoch: [1/10], Loss: 0.0009, Step: 2500\n","Epoch: [1/10], Loss: 0.2059, Step: 2600\n","Epoch: [1/10], Loss: 0.3941, Step: 2700\n","Epoch: [1/10], Loss: 0.0525, Step: 2800\n","Epoch: [1/10], Loss: 0.0030, Step: 2900\n","Epoch: [1/10], Loss: 0.0565, Step: 3000\n","Epoch: [2/10], Loss: 0.0214, Step: 100\n","Epoch: [2/10], Loss: 0.2087, Step: 200\n","Epoch: [2/10], Loss: 0.0255, Step: 300\n","Epoch: [2/10], Loss: 0.0919, Step: 400\n","Epoch: [2/10], Loss: 0.3311, Step: 500\n","Epoch: [2/10], Loss: 0.0162, Step: 600\n","Epoch: [2/10], Loss: 0.0196, Step: 700\n","Epoch: [2/10], Loss: 0.4693, Step: 800\n","Epoch: [2/10], Loss: 0.0099, Step: 900\n","Epoch: [2/10], Loss: 0.0395, Step: 1000\n","Epoch: [2/10], Loss: 0.0097, Step: 1100\n","Epoch: [2/10], Loss: 0.0006, Step: 1200\n","Epoch: [2/10], Loss: 0.0242, Step: 1300\n","Epoch: [2/10], Loss: 0.0023, Step: 1400\n","Epoch: [2/10], Loss: 0.0900, Step: 1500\n","Epoch: [2/10], Loss: 0.0135, Step: 1600\n","Epoch: [2/10], Loss: 0.0027, Step: 1700\n","Epoch: [2/10], Loss: 0.0699, Step: 1800\n","Epoch: [2/10], Loss: 0.0123, Step: 1900\n","Epoch: [2/10], Loss: 0.0023, Step: 2000\n","Epoch: [2/10], Loss: 0.0375, Step: 2100\n","Epoch: [2/10], Loss: 0.0029, Step: 2200\n","Epoch: [2/10], Loss: 0.0677, Step: 2300\n","Epoch: [2/10], Loss: 0.0064, Step: 2400\n","Epoch: [2/10], Loss: 0.0055, Step: 2500\n","Epoch: [2/10], Loss: 0.1209, Step: 2600\n","Epoch: [2/10], Loss: 0.1567, Step: 2700\n","Epoch: [2/10], Loss: 0.0050, Step: 2800\n","Epoch: [2/10], Loss: 0.0684, Step: 2900\n","Epoch: [2/10], Loss: 0.0018, Step: 3000\n","Epoch: [3/10], Loss: 0.2703, Step: 100\n","Epoch: [3/10], Loss: 0.0300, Step: 200\n","Epoch: [3/10], Loss: 0.0074, Step: 300\n","Epoch: [3/10], Loss: 0.0065, Step: 400\n","Epoch: [3/10], Loss: 0.0108, Step: 500\n","Epoch: [3/10], Loss: 0.0132, Step: 600\n","Epoch: [3/10], Loss: 0.3334, Step: 700\n","Epoch: [3/10], Loss: 0.0126, Step: 800\n","Epoch: [3/10], Loss: 0.0036, Step: 900\n","Epoch: [3/10], Loss: 0.0146, Step: 1000\n","Epoch: [3/10], Loss: 0.0008, Step: 1100\n","Epoch: [3/10], Loss: 0.0128, Step: 1200\n","Epoch: [3/10], Loss: 0.0019, Step: 1300\n","Epoch: [3/10], Loss: 0.0013, Step: 1400\n","Epoch: [3/10], Loss: 0.0006, Step: 1500\n","Epoch: [3/10], Loss: 0.0122, Step: 1600\n","Epoch: [3/10], Loss: 0.0651, Step: 1700\n","Epoch: [3/10], Loss: 0.0071, Step: 1800\n","Epoch: [3/10], Loss: 0.0009, Step: 1900\n","Epoch: [3/10], Loss: 0.0002, Step: 2000\n","Epoch: [3/10], Loss: 0.0018, Step: 2100\n","Epoch: [3/10], Loss: 0.0048, Step: 2200\n","Epoch: [3/10], Loss: 0.0009, Step: 2300\n","Epoch: [3/10], Loss: 0.0012, Step: 2400\n","Epoch: [3/10], Loss: 0.0036, Step: 2500\n","Epoch: [3/10], Loss: 0.0590, Step: 2600\n","Epoch: [3/10], Loss: 0.0039, Step: 2700\n","Epoch: [3/10], Loss: 0.0018, Step: 2800\n","Epoch: [3/10], Loss: 0.0806, Step: 2900\n","Epoch: [3/10], Loss: 0.0004, Step: 3000\n","Epoch: [4/10], Loss: 0.0031, Step: 100\n","Epoch: [4/10], Loss: 0.0136, Step: 200\n","Epoch: [4/10], Loss: 0.0421, Step: 300\n","Epoch: [4/10], Loss: 0.1955, Step: 400\n","Epoch: [4/10], Loss: 0.0192, Step: 500\n","Epoch: [4/10], Loss: 0.0009, Step: 600\n","Epoch: [4/10], Loss: 0.0715, Step: 700\n","Epoch: [4/10], Loss: 0.1587, Step: 800\n","Epoch: [4/10], Loss: 0.0099, Step: 900\n","Epoch: [4/10], Loss: 0.0024, Step: 1000\n","Epoch: [4/10], Loss: 0.0231, Step: 1100\n","Epoch: [4/10], Loss: 0.0272, Step: 1200\n","Epoch: [4/10], Loss: 0.0005, Step: 1300\n","Epoch: [4/10], Loss: 0.0022, Step: 1400\n","Epoch: [4/10], Loss: 0.0223, Step: 1500\n","Epoch: [4/10], Loss: 0.0011, Step: 1600\n","Epoch: [4/10], Loss: 0.0029, Step: 1700\n","Epoch: [4/10], Loss: 0.0002, Step: 1800\n","Epoch: [4/10], Loss: 0.0353, Step: 1900\n","Epoch: [4/10], Loss: 0.0026, Step: 2000\n","Epoch: [4/10], Loss: 0.0830, Step: 2100\n","Epoch: [4/10], Loss: 0.0035, Step: 2200\n","Epoch: [4/10], Loss: 0.0022, Step: 2300\n","Epoch: [4/10], Loss: 0.0373, Step: 2400\n","Epoch: [4/10], Loss: 0.0020, Step: 2500\n","Epoch: [4/10], Loss: 0.0064, Step: 2600\n","Epoch: [4/10], Loss: 0.0011, Step: 2700\n","Epoch: [4/10], Loss: 0.0058, Step: 2800\n","Epoch: [4/10], Loss: 0.0235, Step: 2900\n","Epoch: [4/10], Loss: 0.0038, Step: 3000\n","Epoch: [5/10], Loss: 0.0006, Step: 100\n","Epoch: [5/10], Loss: 0.0035, Step: 200\n","Epoch: [5/10], Loss: 0.0019, Step: 300\n","Epoch: [5/10], Loss: 0.0006, Step: 400\n","Epoch: [5/10], Loss: 0.0030, Step: 500\n","Epoch: [5/10], Loss: 0.0290, Step: 600\n","Epoch: [5/10], Loss: 0.0035, Step: 700\n","Epoch: [5/10], Loss: 0.0081, Step: 800\n","Epoch: [5/10], Loss: 0.0011, Step: 900\n","Epoch: [5/10], Loss: 0.0010, Step: 1000\n","Epoch: [5/10], Loss: 0.0004, Step: 1100\n","Epoch: [5/10], Loss: 0.0028, Step: 1200\n","Epoch: [5/10], Loss: 0.0758, Step: 1300\n","Epoch: [5/10], Loss: 0.0300, Step: 1400\n","Epoch: [5/10], Loss: 0.0659, Step: 1500\n","Epoch: [5/10], Loss: 0.0203, Step: 1600\n","Epoch: [5/10], Loss: 0.0001, Step: 1700\n","Epoch: [5/10], Loss: 0.0013, Step: 1800\n","Epoch: [5/10], Loss: 0.0113, Step: 1900\n","Epoch: [5/10], Loss: 0.0383, Step: 2000\n","Epoch: [5/10], Loss: 0.0790, Step: 2100\n","Epoch: [5/10], Loss: 0.0115, Step: 2200\n","Epoch: [5/10], Loss: 0.0013, Step: 2300\n","Epoch: [5/10], Loss: 0.0001, Step: 2400\n","Epoch: [5/10], Loss: 0.0004, Step: 2500\n","Epoch: [5/10], Loss: 0.0036, Step: 2600\n","Epoch: [5/10], Loss: 0.0306, Step: 2700\n","Epoch: [5/10], Loss: 0.0001, Step: 2800\n","Epoch: [5/10], Loss: 0.0079, Step: 2900\n","Epoch: [5/10], Loss: 0.1663, Step: 3000\n","Epoch: [6/10], Loss: 0.0052, Step: 100\n","Epoch: [6/10], Loss: 0.0013, Step: 200\n","Epoch: [6/10], Loss: 0.0001, Step: 300\n","Epoch: [6/10], Loss: 0.0006, Step: 400\n","Epoch: [6/10], Loss: 0.1124, Step: 500\n","Epoch: [6/10], Loss: 0.0127, Step: 600\n","Epoch: [6/10], Loss: 0.0428, Step: 700\n","Epoch: [6/10], Loss: 0.0086, Step: 800\n","Epoch: [6/10], Loss: 0.0130, Step: 900\n","Epoch: [6/10], Loss: 0.0112, Step: 1000\n","Epoch: [6/10], Loss: 0.0017, Step: 1100\n","Epoch: [6/10], Loss: 0.0004, Step: 1200\n","Epoch: [6/10], Loss: 0.0155, Step: 1300\n","Epoch: [6/10], Loss: 0.0003, Step: 1400\n","Epoch: [6/10], Loss: 0.0024, Step: 1500\n","Epoch: [6/10], Loss: 0.0243, Step: 1600\n","Epoch: [6/10], Loss: 0.0003, Step: 1700\n","Epoch: [6/10], Loss: 0.0005, Step: 1800\n","Epoch: [6/10], Loss: 0.0071, Step: 1900\n","Epoch: [6/10], Loss: 0.0001, Step: 2000\n","Epoch: [6/10], Loss: 0.0293, Step: 2100\n","Epoch: [6/10], Loss: 0.0001, Step: 2200\n","Epoch: [6/10], Loss: 0.0002, Step: 2300\n","Epoch: [6/10], Loss: 0.0070, Step: 2400\n","Epoch: [6/10], Loss: 0.0014, Step: 2500\n","Epoch: [6/10], Loss: 0.0086, Step: 2600\n","Epoch: [6/10], Loss: 0.0003, Step: 2700\n","Epoch: [6/10], Loss: 0.0003, Step: 2800\n","Epoch: [6/10], Loss: 0.0032, Step: 2900\n","Epoch: [6/10], Loss: 0.0006, Step: 3000\n","Epoch: [7/10], Loss: 0.2220, Step: 100\n","Epoch: [7/10], Loss: 0.0043, Step: 200\n","Epoch: [7/10], Loss: 0.0006, Step: 300\n","Epoch: [7/10], Loss: 0.0007, Step: 400\n","Epoch: [7/10], Loss: 0.0942, Step: 500\n","Epoch: [7/10], Loss: 0.0030, Step: 600\n","Epoch: [7/10], Loss: 0.0028, Step: 700\n","Epoch: [7/10], Loss: 0.0001, Step: 800\n","Epoch: [7/10], Loss: 0.0009, Step: 900\n","Epoch: [7/10], Loss: 0.0028, Step: 1000\n","Epoch: [7/10], Loss: 0.0013, Step: 1100\n","Epoch: [7/10], Loss: 0.0050, Step: 1200\n","Epoch: [7/10], Loss: 0.0007, Step: 1300\n","Epoch: [7/10], Loss: 0.0012, Step: 1400\n","Epoch: [7/10], Loss: 0.0014, Step: 1500\n","Epoch: [7/10], Loss: 0.0001, Step: 1600\n","Epoch: [7/10], Loss: 0.0010, Step: 1700\n","Epoch: [7/10], Loss: 0.0002, Step: 1800\n","Epoch: [7/10], Loss: 0.0006, Step: 1900\n","Epoch: [7/10], Loss: 0.0252, Step: 2000\n","Epoch: [7/10], Loss: 0.0138, Step: 2100\n","Epoch: [7/10], Loss: 0.0006, Step: 2200\n","Epoch: [7/10], Loss: 0.0106, Step: 2300\n","Epoch: [7/10], Loss: 0.0021, Step: 2400\n","Epoch: [7/10], Loss: 0.0102, Step: 2500\n","Epoch: [7/10], Loss: 0.0100, Step: 2600\n","Epoch: [7/10], Loss: 0.0001, Step: 2700\n","Epoch: [7/10], Loss: 0.0046, Step: 2800\n","Epoch: [7/10], Loss: 0.0027, Step: 2900\n","Epoch: [7/10], Loss: 0.0015, Step: 3000\n","Epoch: [8/10], Loss: 0.0013, Step: 100\n","Epoch: [8/10], Loss: 0.0000, Step: 200\n","Epoch: [8/10], Loss: 0.0007, Step: 300\n","Epoch: [8/10], Loss: 0.0077, Step: 400\n","Epoch: [8/10], Loss: 0.0009, Step: 500\n","Epoch: [8/10], Loss: 0.0046, Step: 600\n","Epoch: [8/10], Loss: 0.0001, Step: 700\n","Epoch: [8/10], Loss: 0.0011, Step: 800\n","Epoch: [8/10], Loss: 0.0019, Step: 900\n","Epoch: [8/10], Loss: 0.0001, Step: 1000\n","Epoch: [8/10], Loss: 0.0081, Step: 1100\n","Epoch: [8/10], Loss: 0.0129, Step: 1200\n","Epoch: [8/10], Loss: 0.0962, Step: 1300\n","Epoch: [8/10], Loss: 0.0006, Step: 1400\n","Epoch: [8/10], Loss: 0.0018, Step: 1500\n","Epoch: [8/10], Loss: 0.0005, Step: 1600\n","Epoch: [8/10], Loss: 0.0048, Step: 1700\n","Epoch: [8/10], Loss: 0.0009, Step: 1800\n","Epoch: [8/10], Loss: 0.0200, Step: 1900\n","Epoch: [8/10], Loss: 0.0002, Step: 2000\n","Epoch: [8/10], Loss: 0.0124, Step: 2100\n","Epoch: [8/10], Loss: 0.0008, Step: 2200\n","Epoch: [8/10], Loss: 0.0021, Step: 2300\n","Epoch: [8/10], Loss: 0.0046, Step: 2400\n","Epoch: [8/10], Loss: 0.0316, Step: 2500\n","Epoch: [8/10], Loss: 0.0078, Step: 2600\n","Epoch: [8/10], Loss: 0.0005, Step: 2700\n","Epoch: [8/10], Loss: 0.0006, Step: 2800\n","Epoch: [8/10], Loss: 0.0006, Step: 2900\n","Epoch: [8/10], Loss: 0.0275, Step: 3000\n","Epoch: [9/10], Loss: 0.0048, Step: 100\n","Epoch: [9/10], Loss: 0.0008, Step: 200\n","Epoch: [9/10], Loss: 0.0071, Step: 300\n","Epoch: [9/10], Loss: 0.0000, Step: 400\n","Epoch: [9/10], Loss: 0.0058, Step: 500\n","Epoch: [9/10], Loss: 0.0008, Step: 600\n","Epoch: [9/10], Loss: 0.0011, Step: 700\n","Epoch: [9/10], Loss: 0.0001, Step: 800\n","Epoch: [9/10], Loss: 0.0026, Step: 900\n","Epoch: [9/10], Loss: 0.0207, Step: 1000\n","Epoch: [9/10], Loss: 0.0023, Step: 1100\n","Epoch: [9/10], Loss: 0.0001, Step: 1200\n","Epoch: [9/10], Loss: 0.0000, Step: 1300\n","Epoch: [9/10], Loss: 0.0024, Step: 1400\n","Epoch: [9/10], Loss: 0.0162, Step: 1500\n","Epoch: [9/10], Loss: 0.0304, Step: 1600\n","Epoch: [9/10], Loss: 0.0000, Step: 1700\n","Epoch: [9/10], Loss: 0.0004, Step: 1800\n","Epoch: [9/10], Loss: 0.0003, Step: 1900\n","Epoch: [9/10], Loss: 0.0040, Step: 2000\n","Epoch: [9/10], Loss: 0.0003, Step: 2100\n","Epoch: [9/10], Loss: 0.0008, Step: 2200\n","Epoch: [9/10], Loss: 0.0000, Step: 2300\n","Epoch: [9/10], Loss: 0.0007, Step: 2400\n","Epoch: [9/10], Loss: 0.0206, Step: 2500\n","Epoch: [9/10], Loss: 0.0055, Step: 2600\n","Epoch: [9/10], Loss: 0.0016, Step: 2700\n","Epoch: [9/10], Loss: 0.0000, Step: 2800\n","Epoch: [9/10], Loss: 0.0046, Step: 2900\n","Epoch: [9/10], Loss: 0.0004, Step: 3000\n","Epoch: [10/10], Loss: 0.0365, Step: 100\n","Epoch: [10/10], Loss: 0.0001, Step: 200\n","Epoch: [10/10], Loss: 0.0041, Step: 300\n","Epoch: [10/10], Loss: 0.0001, Step: 400\n","Epoch: [10/10], Loss: 0.0004, Step: 500\n","Epoch: [10/10], Loss: 0.0038, Step: 600\n","Epoch: [10/10], Loss: 0.0000, Step: 700\n","Epoch: [10/10], Loss: 0.0005, Step: 800\n","Epoch: [10/10], Loss: 0.0289, Step: 900\n","Epoch: [10/10], Loss: 0.0099, Step: 1000\n","Epoch: [10/10], Loss: 0.0004, Step: 1100\n","Epoch: [10/10], Loss: 0.0000, Step: 1200\n","Epoch: [10/10], Loss: 0.0001, Step: 1300\n","Epoch: [10/10], Loss: 0.0005, Step: 1400\n","Epoch: [10/10], Loss: 0.0003, Step: 1500\n","Epoch: [10/10], Loss: 0.0001, Step: 1600\n","Epoch: [10/10], Loss: 0.0006, Step: 1700\n","Epoch: [10/10], Loss: 0.0001, Step: 1800\n","Epoch: [10/10], Loss: 0.0000, Step: 1900\n","Epoch: [10/10], Loss: 0.0027, Step: 2000\n","Epoch: [10/10], Loss: 0.0058, Step: 2100\n","Epoch: [10/10], Loss: 0.0069, Step: 2200\n","Epoch: [10/10], Loss: 0.0000, Step: 2300\n","Epoch: [10/10], Loss: 0.0566, Step: 2400\n","Epoch: [10/10], Loss: 0.0417, Step: 2500\n","Epoch: [10/10], Loss: 0.0001, Step: 2600\n","Epoch: [10/10], Loss: 0.0033, Step: 2700\n","Epoch: [10/10], Loss: 0.0000, Step: 2800\n","Epoch: [10/10], Loss: 0.0122, Step: 2900\n","Epoch: [10/10], Loss: 0.0223, Step: 3000\n"],"name":"stdout"}]},{"metadata":{"id":"-NiW-tTTMD46","colab_type":"text"},"cell_type":"markdown","source":["# Test\n","---\n","이제 얼마나 잘 학습했는지 test를 하면 된다. (Accr : 99.22)"]},{"metadata":{"id":"l2B9oIXosMXy","colab_type":"code","outputId":"54403ed7-6a31-4036-f79a-d864a1cc7567","executionInfo":{"status":"ok","timestamp":1554874854597,"user_tz":-540,"elapsed":3274,"user":{"displayName":"‍신동욱(대학원 컴퓨터공학과)","photoUrl":"","userId":"00553251863555333822"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","  correct = 0\n","  total = 0\n","  for images, labels in test:\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  print('Accuracy test on the 10000 images : {}'.format(100 * correct/total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy test on the 10000 images : 99.22\n"],"name":"stdout"}]}]}