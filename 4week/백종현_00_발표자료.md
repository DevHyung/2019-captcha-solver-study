# MNIST

### 1. 데이터 불러오기

```python
from tensorflow.examples.tutorials.mnist import input_data  #tensorflow샘플에 포함된 예제
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)  #학습데이터 다운로드
import tensorflow as tf
```

+ 55000개의 학습 데이터(mnist.train)
+ 10000개의 테스트 데이터(mnist.test)
+ 5000개의 검증 데이터(mnist.validation)
+ 데이터 = xs(이미지) + xy(라벨)

ex) 학습세트의 이미지 : mnist.train.images

​      학습세트의 라벨 : mnist.train.labels

+ one-hot 벡터는 하나의 차원만 1이고 나머지 모든 차원들은 0으로 채워진 벡터이다.

ex) 3 -> [0,0,0,1,0,0,0,0,0,0]

데이터 갯수가 늘어날수록 벡터의 크기가 계속 늘어나기 때문에 데이터가 많을경우 비효율적이다.



---

### 2. 변수  설정하기

```python
x = tf.placeholder(tf.float32, [None, 784]) # None은 어떤 길이도 될수 있음, 28X28 pixel
W = tf.Variable(tf.zeros([784, 10]))  # 가중치값 변수, 10 : 0~9
b = tf.Variable(tf.zeros([10]))  # 편향값 변수
y = tf.nn.softmax(tf.matmul(x, W) + b)  # matmul 곱하기, 소프트맥스 회귀로 모델 만들기 
```

![softmax-evidence](https://user-images.githubusercontent.com/36959292/55800852-9c835300-5b0f-11e9-868e-0214a36b752a.png)

+ 소프트맥스 방식: 입력이 특정 클래스에 해당되는지 증거를 더하고, 증거를 확률로 변환

증거 계산을 위해서 픽셀 농도의 가중치합을 이용한다.

![softmax](https://user-images.githubusercontent.com/36959292/55801000-04399e00-5b10-11e9-936d-ebdcc05dd35e.png) 

빨간색은 음수, 파란색은 양수 가중치를 갖는다.

+ 입력값을 0~1사이의 값으로 정규화 하여 출력한다. 출력값의 합은 항상 1이다.

ex) (0.9, 0.8, 0.7)가 절댓값은 높지만 (0.6, 0.3, 0.1)이 구분하기 쉽다.



---

### 3. cross-entrophy

모델을 학습시킬때, 무엇이 좋은지 나쁜지 정의가 필요하다.
비용(cost) 또는 소실(loss)을 최소화 하려고 시도하는데 괜찮은 비용함수 중 하나가 cross-entropy이다.
이 값이 낮을수록 좋은 모델이다.

```python
y_ = tf.placeholder(tf.float32, [None, 10])  # 새 placeholder 추가, 숫자 0~9
cross_entropy = -tf.reduce_sum(y_ * tf.log(y))  # 모든 차원이 제거되고 하나의 스칼라 값
```

![cross-entrophy](https://user-images.githubusercontent.com/36959292/55802404-2123a080-5b13-11e9-9ead-fb04c3f664ce.png)



tensorflow는 계산과정의 전체 그래프를 알고 있어, 비용 최소화에 어떤 변수가 얼마나 영향을 주는지 효율적으로 계산한다. cross-entropy(비효율정도)를 최소화하는 방향으로 최적화 하는 방법은 여러가지가 있지만 여기서는
경사하강법(gradient descent)을 이용한다. 

![gradientdecent](https://user-images.githubusercontent.com/36959292/55803668-bfb10100-5b15-11e9-84d6-84da18bb47cf.png)

손실을 줄이기위해 함수의 그래프상(y축 손실) 최소값을 찾기위해 기울기의 반대방향으로 이동한다.
학습도(tensor or floating point value)를 0.01주고 경사하강법알고리즘을 이용하여 교차 엔트로피를 최소화한다.

```python
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
```

이제 학습을 위한 모델의 준비가 끝났다.



---

### 4. 학습시키기

```python
# 만든 변수들을 초기화하고 세션에서 모델을 시작한다.
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
# 1000번을 학습시킨다.
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)  # 학습세트에서 100개의 무작위 데이터들을  가져옴
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})  # 이미지, 라벨
```



---

### 5. 평가하기

```python
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))  #실제와 맞았는지?
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #  타입 캐스트하고, 모든 차원이 제거되고 하나의 스칼라값 평균
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))  # 테스트 이미지와 라벨을 넣고 정확도 테스트
```

+ tf.argmax() : 가장 큰 원소의 인덱스



---

### 6. 참고

<https://bcho.tistory.com/1154>

<https://codeonweb.com/entry/12045839-0aa9-4bad-8c7e-336b89401e10>

 <https://www.tensorflow.org/tutorials/mnist/beginners/>

