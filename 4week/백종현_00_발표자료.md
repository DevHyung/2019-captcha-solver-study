#### 1. Machine Learning의 용어와 개념

---

supervised learning : 정해진 데이터로 학습(with labeled examples - training set)

- Image labeling
- Email spam filter
- Predicting exam score

unspervised learning : 데이터를 보고 스스로 학습 un-labeled 데이터 (Google news, Word clustering) 



##### Training data set

x = 3, 6, 9 y = 3

x = 2, 5, 7 y = 2

x = 2, 3, 5 y = 1

학습하여 모델 생성

test x = 9, 3, 6 -> 모델 - > y ?



##### Types of supervised learning

predicting exam score

0~100점 : regression

P or F : binary classification

A, B, C, D, F : multi-label classification



x(hours) y(score)

10		90

9		  80

3		  50

2		  30

학습하여 모델 생성

test x = 7 -> regression모델 -> y?



#### 2. Linear regression

---

Regression data

x = 1 y = 1

x = 2 y = 2

x = 3 y = 3



(Linear) Hypothesis

어떤 선이 데이터에 가장 잘 맞는가 찾기(W,b)

H(x) = Wx + b

H(x) = 0.5x + 2   vs   H(x) = 1x + 0



##### Cost function

가설과 실제 데이터가 얼마나 다른가?

(H(x) - y)^2 (+인지 -인지 상관없이 표현 가능)

각 데이터에 대한 차이의 평균을 구하여 비교

![캡처](https://user-images.githubusercontent.com/36959292/56089769-0fefe080-5ed3-11e9-9621-0ea738ed5aab.PNG)



##### Goal

minimize cost(W,b)

가장 적은 값을 갖는 W, b 찾기



#### 3. cost 최소화 알고리즘

---

각 W에 대한 cost(W) 그래프에서 최솟값을 찾아야 한다.

-> Gradient descent algorithm(경사하강법)

어떻게 찾을까??

W와 b를 조금씩 바꾸면서 cost(W)를 미분(기울기)하여 경사로쪽 즉, cost의 최소로 감

![캡처1](https://user-images.githubusercontent.com/36959292/56089900-0e271c80-5ed5-11e9-95de-659a65e69b9c.PNG)

#### 4. multi-variable linear regression

---

quiz1, quiz2, exam1의 점수로 final 점수 예측하기

H(x1,x2,x3) = w1x1 + w2x2 + w3x3 + b

변수가 많아졌을 때 잘 처리하기 위하여 Matrix를 이용

![캡처2](https://user-images.githubusercontent.com/36959292/56090077-f735f980-5ed7-11e9-8029-b37360fdac91.PNG)



#### 5. Logistic classification

---

Spam detection : Spam(1) or Ham(0)

Facebook feed : show(1) or hide(0)

exam : pass(1) or fail(0)

학습의 결과로 H(x) = 0.5x라하자. x = 100 이면 50 즉, 1보다 매우크다

그래서 0~1의 값을 갖도록 새로운 함수를 도입 -> sigmoid function(logistic function)

![캡처3](https://user-images.githubusercontent.com/36959292/56090264-f0f54c80-5eda-11e9-86a0-04372afb48c0.PNG)



 #### 4. Logistic Regression의 cost함수

---

기존 sigmoid의 cost함수는 local minimum은 찾지만 global minimum은 찾기 힘들다.

새로운 cost 함수

![캡처4](https://user-images.githubusercontent.com/36959292/56090686-44b66480-5ee0-11e9-91bb-7ac33d71a2f6.PNG)

y = 1일때 H(x) = 1 이면 cost(1) = 0 -> 예측 성공, H(x) = 0 이면 cost(0) = 무한대 

y = 0일때 H(x) = 0 이면 cost(0) = 0 -> 예측 성공 , H(x) = 1 이면 cost(1) = 무한대 



#### 5. softmax

---

x -> a or not -> y_

x -> b or not -> y_

x -> c or not -> y_

각각의 logistic classification를 하나의 벡터로 만들어 행렬 연산 가능

여러개 중에서 하나를 예측할 것이 있을 때 softmax를 쓴다.

a : 2.0 b : 1.0 c : 0.1

0~1사이의 값으로 정규화 + 합이 1 -> softmax

a : 0.7 b : 0.2 c : 0.1

one-hot encoding을 하면 1 0 0 즉 a가 나온다.